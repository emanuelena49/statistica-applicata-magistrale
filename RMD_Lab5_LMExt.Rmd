---
title: "Lab 5 - Towards multiple linear regression and logistic regression"
author: "Emanuele Lena"
date: "8/11/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# La regressione lineare multipla

Obiettivo: studio del modello di regressione lineare multipla

* generaliz. modello semplice
* alcune questioni più generali 

Differenza con caso semplice: ammettiamo più previsori (var. esplicative, covariata)

Perché è importante? Non modella troppe situaz. reali ma si imparano alcune cose di carat.generale

$y_i = \beta_0 + \beta_1*x_{i1} + ... + \beta_p*x_{ip} + \epsilon_i $

...

possiamo esprimere tutto in termini matriciali

$Y=X\beta+\epsilon$

X = (
1 x11 ... x1p
.  .
.  .
.  .
1 xn1 ... xnp
)

<- può includere tante cose:
* semplici variabili numeriche
* fattori (categoriali)
* altri regressori
* ...

beta <- vett. colonna $(\beta_0,\beta_1, ... , \beta_p)^T $

epsilon <- vett. colonna $(\epsilon_1, ... , \epsilon_n)^T $

Y <- vett. colonna $(y_1, ... , y_n)^T $

### La gestione dei fattori

Solitamente si effettua inserendo, per ongi livello una "variabile dummy" binaria in $\{1,0\}$ per ogni livello. 

e.g., 

y descritto da 2 var numeriche $v,u$ più un fattore $g$ con 3 livelli:

* in X, elimino la colonna 0
* introduco 3 variabili dummy binarie, che si attivano quando un'oss. appartiene ad una certa categoria
* per ongi nuova colonna, introdurrò una nuova $\beta_0$ legata alla categoria (?)

Oss. su slide 9 <- [...]


## La stima del modello

...

vettore beta <- stimato con metodo degli scarti quadrati

$\beta cappello = (X^TX)^{-1}X^Ty$

essendo t. lineare di vettore randomico, segue  $N_{p+1}(\beta, V(\beta))$

con $V(B) = \sigma^2(X^TX)^{-1}$


$y cappello = vett. colonna 1...n = \mu cappello = X*\beta cappello $

giocando con matrici ottengo che $y cappello = ... = H*y$, con $H =X(X^TX)^{-1}X^T$

proprietà di H:

* traccia = p+1
* H*H = identità (?)

$(\beta_jcappello - \beta_j)/SE(\beta_jcappello)$ è una t di student con n-p-1 gradi di libertà
=> posso in questo modo trovare intervalli di confidenza su beta_j

## Test vari

### Test T su beta

interpretaz. risultati inferenziali su beta

* singolo test t su singolo j <- valuta l'ipotesi che UNO dei beta sia nullo (tutti gli altri si considerano nel modello)
* ricorda che i regressori NON è detto che siano indipendenti tra di loro => ogni volta che tolgo un termine di regressione devo stimare il modello, perché i vari regr. potrebbero avere "dipendenza"
* p-value <- danno idea su rilevanza singolo regressore, ma vedremo che in certi casi non possiamo fare troppo affidamento su di esso [...]

### Test F su beta

test locale, 

ipotesi nulla: modello nullo (tutti i beta = 0)
ipotesi 1: almeno un beta != 0

(NOTA: oss. che, al contrario del caso lineare semplice, non c'è corrisp. tra T e F)

### I residui

residui stimati <-$\epsilon cappello = y - y cappello = vett. colonna 1...n$

[...]

### Int. confidenza su beta 

Int. di confidenza per i singoli beta <- [...]

n-p-1 gradi libertà

### Int. confidenza sulle medie

[...]

Oss. che mettendo p a 0 mi riconduco a caso semplice (è una generalizzazione)

### Int. confidenza su previsore

Ricorda: 

* media yi = stimatore = valore previsto
* la varianza però deve includere anche il termie d'errore 

=> int. confidenza su intervallo più ampio 

## Install the packages "DAAG", "MASS", "lattice", "compositions", "ISLR", "car" ##

```{r}
# install.packages("DAAG")
# install.packages("MASS")
# install.packages("lattice")
# install.packages("compositions")
# install.packages("ISLR")
# install.packages("car")
```


# Multiple linear regression with continuous responses

## Example: book weight

Alcune info su libri

* volume
* area copert.
* peso
* tipo copertina (rigida e flessibile)

```{r}
library(DAAG)
library(lattice)

```



### Analisi preliminare 

Prima cosa interessante: fare una serie di scatterplot per mettere in relazione le varie var. numeriche

```{r}
pairs(allbacks[,1:3], # selez. volume, area e peso
      lwd=2)
```

Per rappr. più sofisticata con funz. di lattice: 
```{r}
splom(~allbacks[,1:3], # vuole in input ogg. cls formula
      lwd=2, 
      groups = cover, # evidenzia anche il fattore "cover" con colore
      auto.key = T, 
      data = allbacks)

```

### Modello

Cerco di spiegare il fatt. peso con il volume e l'area:

```{r}
allbacks.lm <- lm(weight  ~ volume + area, data=allbacks)
summary(allbacks.lm)
```

Oss: comando + summary <- molto simili a caso semplice 

per ogni beta ho: un valore stimato, lo standard error, il t-value  e il p-value

* per beta 0 osservo p.value molto elevato => potrei voler rimuovere l'intercetta, ma non mi da fastidio (inoltre sarebbe scomodo perché dovrei re-interpr. tanti strumenti)

* vol e area (beta 1 e 2) hanno p-vale alti


Poi oss. 
* SE dei residui
* R^2 
* statistica F per vedere che almeno uno dei due beta 1,2 risulti diverso da 0 (p-value basso <- almeno un beta è diverso da 0)

osservo che test F NON corrisp. a test su beta (in questo caso)

Potrebbe capitare di avere p-value basso per F e p-value alto per i singoli beta <- (tante situazioni particolari, vedremo dopo ...)


### Anova 

(in questa situaz. ) ci da un output di tab. di analisi della varianza

```{r}
anova(allbacks.lm)
```

Oss: 

* il p-value dell'area corrisp. con prec MA quello del volume no

Come interpretare questa tabella?

Essa mette in campo ulteriori test F.,

PRimo test F mette a confronto:

* il modello nullo
* il modello con un unico regressore (volume)

perchè il p-value del volume non equivale con test T? perchè in test T il regressore dell'altra variabile (area) rimane. In questo caso, valuto in isolamento il volume

E il secondo test F? INCREMENTALMENTE metto a confronto:

* modello con solo volume
* modello con area e volume

IN QUESTO CASO, IL TEST CORRISPONDE CON IL T DI PRIMA, perché considero (come prima) entrambi le variabili.

Cosa posso concludere: il volume influenza come risposta ma influisce anche essendo a sua volta un regressore per area

(EFFETTO DI CO-LINEARITA') <- vedremo come valutare

```{r}
cor(allbacks$volume,allbacks$area)

```

(Difatti calcolando la corr. semplice osservo )

```{r}
library(car)
car::vif(allbacks.lm)
DAAG::vif(allbacks.lm)
```
### Diagnostiche

Anche qui posso introdurre diagnostiche (come per reg. semplice)

Come per lm semplice, uso plot diret. su modello, utilizzando which per selezionare cosa mostrare

```{r}
par(mfrow=c(2,2))

# residui vs val. stimati 
plot(allbacks.lm, which = 1, lwd=2, pch = 16, cex.caption=1)

# QQ (quantili teorici e val. residui)
plot(allbacks.lm, which = 2, xlab="Theoretical quantiles",lwd=2, pch = 16, cex.caption=1)

# valori stimati e rad. residui (Residui standard. in val assoluto)
plot(allbacks.lm, which = 3,lwd=2, pch = 16, cex.caption=1)

# dist. di cook
plot(allbacks.lm, which = 4,lwd=2, pch = 16, cex.caption=1)
par(mfrow=c(1,1))
```


## Example: hill races

23 corse (in collina) in 23 diverse località

* dist percorsa
* dislivello
* record maschile e femminile


Come prima, costruiamo due var. di scatterplot 

```{r}
library(DAAG)
library(lattice)
gr1 <- splom(~nihills[,c("dist","climb","time")],lwd=2)
gr2 <- splom(~log(nihills[,c("dist","climb","time")]),
             varnames=c("log dist", "log climb", "log time"),lwd=2)
print(gr1, position=c(0, 0, 0.5, 1))
print(gr2, position=c(0.5, 0, 1, 1),  newpage=FALSE)
```

Osservando la scala normale: vedo concentrazioni particolarmente "dense" con pochi valori molto lontani => provo a passare alla scala log

<- le rel. appaiono più lineari

=> applichiamo reg. lineare multipla ma su log dati: 

spiego tempo con dist. e dislivello

```{r}
nihills.lm <- lm(log(time) ~ log(dist) + log(climb), data = nihills) # mod. in scala log
summary(nihills.lm)
```

Sia test F che T sono signif. => ok


```{r}
par(mfrow=c(2,2))
plot(nihills.lm, which = 1, lwd=2, pch = 16, cex.caption=1)
plot(nihills.lm, which = 2, xlab="Theoretical quantiles",lwd=2, pch = 16, 
     cex.caption=1)
plot(nihills.lm, which = 3,lwd=2, pch = 16, cex.caption=1)
plot(nihills.lm, which = 4,lwd=2, pch = 16, cex.caption=1)
par(mfrow=c(1,1))
```

(diagnostiche <- pt. un po' particolari ma okay, son dovuti a bassa numerosità dei dati)

### Interpr. dei log dati e valutazione di cambio modello

Ora il problema è l'iterpretazione. 

Cosa significa usare i log dati? 

$log(tempo) = \beta_0 + \beta_1*log(dist) + \beta_1*log(dist)$

=> ...

$time = 0.0007 * dist^{0.68} * climb^{0.47}$

Sembra che aumentando la distanza, la media aumenta....

MA ricordo che sto tenendo fisso il dislivello! => 

ci sta che a parità di dislivello, aumentando la distanza aumenta anche la velocità media (meno pendente => corro più veloce)

QUESTO POTREBBE DARE DIFFICOLTA' DI INTERPRET. 

=> potrei modificare il secondo regressore, ed esprimerlo come log(climb/dist)

<- mantengo tutti i test di prima (...) ma rendo il tutto più facilmente

Ora costruiamo:

```{r}

# applico log a tutti dati
lognihills <- log(nihills)

# incollo "log" al nome di ogni colonna (per chiarezza)
names(lognihills) <- paste("log", names(nihills), sep="")
str(lognihills)
```


```{r}
# creo una nuova colonna, cioè la diff. tra climb e dist (in scala log)
lognihills$logGrad <- with(nihills, log(climb/dist))

# costruisco il nuovo modello 
nihillsG.lm <- lm(logtime ~ logdist + logGrad, data=lognihills)
summary(nihillsG.lm)
```

Oss. che st. sono le stesse


```{r}
with(lognihills, cor(cbind(logGrad, logclimb), logdist))

```



# Teoria 11/10

## [...]

## La stima della bontà di un modello

### R^2

[...]

### La selezione di modelli (backward selection e forward selection)

modello annidato <- modello composto da modelli (azzerando certi beta, ottengo i sotto-modelli)

dati p regressori <- tantissime combinazioni.

Test F (con ANOVA) su mod.annidato <- permette di individuare quali dei sotto-modelli sono più significativi 

backward selection:
* si prende un modello abbastanza complesso (p molto alto)
* lo si verifica
* si prova poi ad applicare vari sotto-modelli

Alternativa (forward selection): parto da modello abbastanza semplice, aggiungo proggressivamente complessità. Test F <- ripetuti finché l'aggiunta di termini non da più controb, significativi 

### Confronto di modelli non-annidati (AIC statistic, Mallow's Cp )

No test F, altri criteri

AIC statistic, Mallow's Cp <- [...]

Anche qui posso applicare strategie backward e forward (tolgo e aggiungo regressori asseconda di cosa i criteri ci suggeristico)


### Metodi computazionali 

* metodi di tipo lasso <- selez. automatica dei reg. rilevanti (idea: stima modello = massimizzazione di f. obiettivo. In questo caso, si massimizza f.obiettivo ma con penalizzazione, che disincentiva stima dei coefficenti != 0 => molti beta risulteranno nulli, emergeranno solo quelli realmente rilevanti)

* procedure di boosting <- come se avessimo tanti modelli di regressione con un unico modello. [...] importante: fermare alg. in momento opportuno da selez. solo reg. più rilevanti

In generale,è sempre importante: selez. un modello che faccia fit MA che non cada nell'overfitting


## (esempio continua)

### Il grafico dei residui parziali 

faccio rif. a dati su scala logaritmica

Voglio ri-scrivere i regressori ri-centrandoli rispetto alla media

```{r}

# ri-scrittura del modello con i regressori ricentrando risp. alla media
nihillsG.lm_demean <- lm(logtime ~ 
        I(logdist-mean(logdist)) + 
        I(logGrad-mean(logGrad)), 
        # (NOTA: I(...) <- modo di fare questa op. nella def. del modello
        # avrei anche potuto solo aggungere due colonne)
        
        data=lognihills)


summary(nihillsG.lm_demean)
```

Termplot <- Fai il plot dei residui parziali, in base ad un singolo termine

```{r}
nihills.lm <- lm(logtime ~ logdist + logclimb, data=lognihills)
par(mfrow=c(1,2))

# termplot per primo regressore
termplot(nihills.lm, # ogg: <- modello di partenza (la ri-centratura la fa R)
         terms=1, # su quale dei regressori voglio fare il plot?
         partial.resid=TRUE, # voglio rappr. con punti i residui
         se=T, # disenga anche la banda degli intervalli di stima
         
         lwd.term=2,lwd.se=2,pch=20,
         smooth=panel.smooth, col.smth='blue', col.res="gray30")


# ripeto per secondo regressore (sta volta tralascio le bande di conf.)
termplot(nihills.lm, terms=2,partial.resid=TRUE, lwd.term=2,lwd.se=2,pch=20,
         smooth=panel.smooth, col.smth='blue', col.res="gray30")
par(mfrow=c(1,1))
```

Legenda primo grafico: 
* rette tratt gialle <- bande di confidenza
* retta rosssa <- retta di regressione del modello
* retta tratteggiata blu <- pattern ottenuto con lowess
* puntini grigi <- residui

Come interpretare:
* ciò che il regressore non ha spiegato logclimb, viene spiegatda logdist (e vice versa)
* nel secondo, ci sono variazioni più forti
* nel primo, tanti puntini sono fuori dalle bande di confidenza


### Diagnostiche del modello stimato

(classici plot delle diagnostiche del modello, selez con which)

```{r}
par(mfrow=c(1,3))

# dist di cook 
plot(nihills.lm, which=4, lwd=2, pch = 16, cex.caption=0.8)

# residui ed effetto leva
plot(nihills.lm, which=5, lwd=2, pch = 16, cex.caption=0.8)

# residui ed effetto leva (2)
plot(nihills.lm, which=6, lwd=2, pch = 16, cex.caption=0.8)
par(mfrow=c(1,1))
```

Oss: 

guardando grafici, si oss. tre gare che possono essere potenziali anomalie. Sono outlyer?

Voglio approfondire. Uso questi due strumenti:

* dfbetas <- genera sequenza dove ho valori riferiti a ogni beta (per ogni osservazione)
* influence.measures <- generalizz di dfbetas


### Misura degli scarti per gli stimatori di beta (dfbetas)


dfbetas <- genera sequenza dove ho valori riferiti a ogni beta (per ogni osservazione)

cosa sono questi valori? Differenza tra stima di beta con tutti i dati e stima di beta senza l'osservazione indicata => posso capire quanto lasciare/togliere un dato influenza le stime

(NOTA: la diff. viene poi standardizzata per uno std. error)

```{r}
dfbetas(nihills.lm)
```

Come valutiamo?

gauss. stadard <- val estremi: oltre 2 e sotto -2

=> guida pratica: cercare valori vicini a 2 per cercare punti particolarmente rilevanti

Qui oss. che anche l'oss. che prima abbiamo individuato come "peggiore", è ampiamente sotto 2.

### influence.measures e fattori di leverage

influence.measures <- prime tre colonne come dfbetas, poi aggiunge (partendo dal fondo):

* inf <- indica punti potenzialmente influenti (con *)
* hat <- fattori di leverage 
* dist. di cook
* cov.r <- confronto variabilità stime con osservazione e senza osservazione
* dffit <- differenza tra i vari y cappello stimati con e senza oss.
* (ultimi tre <- stessi valori di beta)

```{r}
influence.measures(nihills.lm)
```

(conclusioni<- simili a quelle di prima, non ci sono valori strani)

### Covarianti quadratiche (e modelli polinomiale)

Introduc. un termine quadratico al modello 

```{r}

# modello originale
nihills.lm <- lm(logtime ~ logdist + logclimb, data = lognihills)

# modello con t. quadratico
nihills2.lm <- lm(logtime ~ logdist + logclimb + I(logdist^2), data = lognihills)


summary(nihills.lm)
summary(nihills2.lm)
```

Oss. 
* in mod. quadratico, leggero miglioramento per R^2
* test T abbastanza simile per pendenza
* test T per reg. quadratico <- molto alto (8%) per ^2, lo stesso alto per logdist

=> mi verrebbe da dire tolgo log dist e log dist ^ 2. NOTA: questo però può essere dovuto al fatto che c'è una forte correlaz. per i due regressori 

=> servono altri confronti: 

* R^2 <- ok
* AIC e BIC (più bassi sono, meglio è)

```{r}
AIC(nihills.lm,nihills2.lm)
BIC(nihills.lm,nihills2.lm)
```

In entrambi i casi, il t. quadratico risulta vantaggioso.


Esercizio: prova a rimuovere logdist (non quadrato) dal modello.

# 17/11/2021

## Example: cars


```{r}
plot(dist ~ speed, data = cars, xlim=c(3,1.04*max(speed)),
     ylim=c(0,1.04*max(dist)), xlab = 'Speed', ylab = 'Distance', pch = 16)
```


### Impl. di un modello polinomiale

Andiamo a costruire un modello dove considero un nuovo regressore (il quadrato)

```{r}
cars2.lm <- lm(dist ~ speed + I(speed^2),data=cars)

```


Ora voglio sovrapporre al plot il mio modello quadratico. Come faccio?


Alternativa 1:
```{r}
# specifico una serie di valori equispaziati per il reg. speed
dat <- data.frame(speed = seq(3,25,length=100)) 
# uso i valori per fare una previsione
fv <- predict(cars2.lm,newdata=dat,se=TRUE) # (se=TRUE <- calcola anche std. error)

# elemento fit <- contiene medie stimate
fv$fit
```


Alternativa 2: 
```{r}
# prendo stime dei coefficienti e le moltiplico per valori x (applico manualmente il modello)
fitvalues <- cars2.lm$coef[1] + cars2.lm$coef[2]*seq(3,25,length=100) + 
       cars2.lm$coef[3]*seq(3,25,length=100)^2
fitvalues
```




```{r}
plot(dist ~ speed, data = cars, xlim=c(3,1.04*max(speed)),
     ylim=c(0,1.04*max(dist)), xlab = 'Speed', ylab = 'Distance', pch = 16)

# sovrascrivo scatterplot con nuovi valori (come linea)
lines(dat$speed,
      fv$fit,
      lwd=2, 
      col='red')

# (potrei essere interessato a confrontare questo con pattern suggerito dai dati)
with(cars, lines(lowess(dist ~ speed, f=.7), lwd=2, col='blue'))
```


```{r}
summary(cars2.lm)

```
Oss.

* per discorso co-linearità, non posso fidarmi del test T
* (vedremo come trattare, eg. vif)

```{r}
library(DAAG)
vif(cars2.lm)
```
vif > 5 --> indice di co-linearità (vedremo meglio in seguito)


### Diagnostiche

```{r}
par(mfrow=c(2,2))
cars2.lm <- lm(dist ~ speed + I(speed^2),data=cars)
plot(cars2.lm, which = 1, lwd=2, pch = 16, cex.caption=0.8)
plot(cars2.lm, which = 2, xlab="Theoretical quantiles",lwd=2, pch = 16, 
     cex.caption=0.8)
plot(cars2.lm, which = 3,lwd=2, pch = 16, cex.caption=0.8)
plot(cars2.lm, which = 4,lwd=2, pch = 16, cex.caption=0.8)
par(mfrow=c(1,1))
```

Cosa osservo?

* man mano che cresce speed, la variabilità non è costante 

<- cattivo segnale


Idea: considerare i minimi quadrati pesati:
pesare per il reciproco di speed
```{r}
cars2w.lm <- lm(dist ~ speed + I(speed^2),
                data=cars, 
                # specifico che i pesi che voglio avere sono il reciproco della velocità
                weights=1/speed  
                )


summary(cars2w.lm)
```

Oss. le diagnostiche mi accorgo che ho un miglioramento [...]

### Confronto tra modelli (annidati e non)

Parto def. alcuni modelli alternativi 
```{r}
cars0.lm <- lm(dist ~ speed, data = cars) # solo speed

# solo speed al quadrato (senza considerare l'intercetta 
# e con il metodo dei minimi quadrati pesati)
cars1w.lm <- lm(dist ~ I(speed^2) -1, data=cars, weights=1/speed) 

# mod precedente (...)
cars2w.lm <- lm(dist ~ speed + I(speed^2), data=cars, weights=1/speed)

```

#### Confronto tra due modelli con test F e ANOVA
Ora confronto due mod. nested per volta:
```{r}
# cars0.lm è incluso in cars2w.lm
anova(cars0.lm,cars2w.lm)
```

Test F (non specifichiamo la statistica test ma la logica)

* i dati favoriscono chiaramente il modello due

```{r}
# cars1w.lm è incluso in cars2w.lm
anova(cars1w.lm,cars2w.lm)
```

* in questo caso, i dati favoriscono sempre il modello due, ma con meno certezza 
(p-value circa = 5%)


#### Confronto con drop 1

* parto da modello full
* valuto cosa succede quando tolgo una covariata

(vice-versa head1 calcola l'inserimento di una covariata alla volta)

```{r}
drop1(cars2w.lm, test = "F")

# NOTA: potrei anche "focalizzare" l'attenzione su alcuni regressori 
# specificando il parametro scope=...



# NOTA2: potrei anche voler specificare il test, k (peso del termine di penalizzazione 
# dei criteri di tipo akaike - eg. k=log(n) <- BIC)
```

Oss. tra le covariate conviene escludere questa specifica (speed)

* valori AIC <- più basso è, meglio è (in questo caso, il risultato migliore si ottiene senza speed)
* come P-value, osserviamo che i valori corrisp. a quelli del test T. Perché?

Perchè vado a testare, con proc. analoghe, le stesse ipotesi (beta=0, beta!=0 mantenendo gli altri regressori)


#### Alcune considerazioni su AIC

```{r}
AIC(cars0.lm,cars1w.lm,cars2w.lm)
```

Oss: I valori AIC che vedo qua sono diverse da quelle di drop1, ma non dobbiamo preoccuparci

La cosa rilevante è il confronto tra i valori (non i val. stessi che possono essere calcolati con metodi diversi)

DOMANDA: perchè usando l'AIC nel drop1 si preferisce il mod1 e qui invece si preferisce il modello2? 
ATTENZIONE A COSA STIAMO CONFRONTANDO: qui sto confrontanto un mod. diverso! Qui NON considero l'intercetta 


```{r}
BIC(cars0.lm,cars1w.lm,cars2w.lm)

```

Oss: applicando BIC il valore preferito è l'1 (non il due dell'AIC)

Filosofia di BIC <- penalizza modelli complessi

#### Summary sui 3 mod. concorrenti e conftonto su R^2

```{r}
summary(cars0.lm)
summary(cars1w.lm)
summary(cars2w.lm)
```

[...] (fai da solo)


# Covariates: selection and multicollinearity

## Consideraz. varie

Spesso, si raccolgono molte potenziali variabili esplicative dei dati (spesso sono troppe)

esempio: ho una variabile che voglio spiegare e 20 covariate. Come mi comporto?

1) conoscenza degli esperti (quali var. mi aspetto siano importanti?)

2) quanti dati voglio avere se voglio stimare un modello con N covariate?

buona norma: osservazioni >= 10*N covariate

putroppo una situaz. tipica è oss. <= covariate

3) visualizzazione grafica (matrice con i vari scatterplot)

4) cercare di evitare co-variate che danno info equivalenti 

5) attenzione a "correlazioni spurie" 

(apparenti) correlazioni tra variabili:

* completamente scorrelate ma che per caso hanno andamenti simili
* 2 variabili entrambe correlate con una terza molto generica

(vedi esempi simpatici del sito)

### I metodi

Abbiamo visto alcuni metodi moderni: LASSO, ...

Altra idea: aggregare tante covariate in poche (ovviamente così semplifico la modellazione ma perdo capacità esplicativa)

## Il problema della multi-collinearità 

più covariate portano informazioni sovrapposte, quando due covariate sono a loro volta correlate 

(succede specie nelli studi osservazionali <- dove osservo cose "in natura", senza predisporre l'esperimento)

Come misurare? 

* indice VIF (fattore d'inflazione della varianza) <- 

$VIF_j = 1/(1-R_j^2)$

$R_j^2$ <- coeff. di correlaz. lineare dove la risposta è variabile e le altre sono i potenziali regressori.


=> VIF alto -> elevata collinearità 

* circa 4,5 <- zona critica
* > 10 <- forte
* circa 1 <- regressori praticamente ortogonali

## Example: coxite

25 oss, campioni di roccia.

tante variabili: 

* percentuali di 5 minerali (A,B,C,D,E)
* profondità
* porosità della roccia 

NOTA: A+B+C+D+E = 100% <- gia qui capisco che potenzialm. potrei toglierne una 

OBIETTIVO: studio della porosità tenendo conto delle altre

```{r}
library(compositions)
data(Coxite)
coxite <- as.data.frame(Coxite)
pairs(coxite)
```


```{r}
coxiteAll.lm <- lm(porosity ~ A+B+C+D+E+depth, data=coxite)
summary(coxiteAll.lm)
```

OSS.ni:

* tutti i test T alti <- nessun regressore va benissimo
* R scarta E perchè si accorge che è totalmente dipendente dagli altri
* test F molto basso <- non conviene il mod. nullo

Problema: tutti i T sono alti, che faccio? Quali tengo?


Proviamo a togliere E:
```{r}
coxite0.lm <- lm(porosity ~ A+B+C+D+depth, data=coxite)
summary(coxite0.lm)
```


Proviamo il test VIF:


```{r}
library(DAAG)
vif(coxite0.lm)
```


A parte per depth, il regressore assume valore molto alto (alta correlaz. tra di loro)

Proviamo a calcolare la correlaz. tra la la porosità e gli altri parametri


```{r}
cor(coxite$porosity, coxite[,-7])

```

Oss. valori elevati (positiv. e negativ.) per A,B,C => penso a tenerli 

=> costruisco un modello con A, B, C:

```{r}
coxite1.lm <- lm(porosity ~ A+B+C, data=coxite)
summary(coxite1.lm)
vif(coxite1.lm)
```


Dal p-value del test T sembra, che A sia un buon candidato per essere scartato:

=> costruisco un nuovo modello con solo B e C

```{r}
coxite2.lm <- lm(porosity ~ B+C, data=coxite)
summary(coxite2.lm)
vif(coxite2.lm)
```


Ora facciamo un confronto tra i modelli 

```{r}
AIC(coxite0.lm,coxite2.lm)
AIC(coxite1.lm,coxite2.lm)

```

Oss:

il mod. 2 sembra essere il più robusto (anche per la sua semplicità)

(NOTA: avendo poche covariati, potrei valutare anche analisi più approfondite in questo caso)

[...]

# Factors as explanatory variables (introduzione di variabili esplicative categoriali)


[...]


## Analisi della varianza a più vie (ANOVA 2 WAYS)

Il fatto che due fattori siano presenti assieme, per la loro iterazione possono influenzare la variabile risposta

### Es. le rotture del filo

Var risposta numerico, più fattori

n.rotture filo su lungh. fissa di filo

fattori <- due lv. per tipi di lana, tre. lv di tensione

=> 6 gruppi

9 osservazioni.

[... tab. da slide 58 ... ]

Partiamo da due plot (uno per A e uno per B)


Proviamo a "unire" i valori medi per ogni valore (L,M,H)

sovrapponiamo i grafici di A e B. 

Oss. che non c'è solo un effetto diretto di "lana" e "tensione", ma anche un effetto dato dall'iterazione dei due fattori


## Il modello per l'ANOVA a due vie

Senza iteraz. tra i due fattori, mi aspetto - per ogni fattore - la stessa influenza indipendentemente dal valore dell'altro fattore

modello: 

$y_{ijk} = \mu + \alpha_i + \beta_j + \epsilon_{ijk}$

$\alpha_1 , ... , \alpha_a $ <- influenza dei valori 1...a dei del primo fattore

$\beta_1 , ... , \alpha_a $


### Il test

H0: $\alpha_1 = ... = \alpha_a = 0$
H1: almeno uno è diverso da 0


### E per l'effetto itarazione?

Introduco $\gamma_{ij}$ nel modello [...]


## Considerazioni


* gerarchia di analisi. Di solito si parte sull'analisi dei gamma (se c'è l'iterazione, probabilmente ci saranno anche gli effetti separati)
* [...]
* p-value 


## [...]



## Example: paper resistance ##

Dataframe di due elementi: resistence e trt

* res. <- numerico
* trt <- categoriale 

```{r}
paper <- data.frame(resistance =
c(7, 8, 15,  11, 9, 10,# 5%
12, 17, 13, 18, 19, 15,# 10%
14, 18, 19, 17, 16, 18,# 15%
19, 25, 22, 23, 18, 20), # 20%
trt = rep(c("5%", "10%", "15%", "20%"),
c(6, 6, 6, 6)))
```

Forzo l'interpr. di trt come fattore
(implicitamente gli dico che il primo è il livello di riferimento https://it.wikipedia.org/wiki/Valore_p )

```{r}
paper$trt <- factor(paper$trt,levels=c("5%", "10%", "15%", "20%"))
# paper$trt <- relevel(paper$trt, ref="5%") # an alternative in order to specify the reference level
```


Cassico testi di varianza ad una via:
```{r}
paper.aov <- aov( resistance ~ trt , data=paper)
summary(paper.aov)
```

Oss:
* ip. nulla pu essere ignorata (p-value molto basso, almeno una var espl != 0)



### Approccio lm

Posso usare anche appr. lm:
Approccio lm vs approccio aov <- lm è più generale, posso anche fare analisi con var espl. miste


specifichiamo un mod. lineare (anche con categoriale)


Cosa fa R?
* vede trt come categoriale
* vede 4 lv
* considera 5% come "reference level"
* tante var dummy tante quante i lv. - 1

```{r}
paper.lm1 <- lm(resistance ~ trt,data=paper)
summary(paper.lm1)
```





```{r}
summary.lm(paper.aov)

```

Oss.

* beta 0 stimato a 10 <- è l'intercetta
* beta 1,2,3 sommate a beta 0 danno la stima della media a 10%, 15%, 20%
* test F <- mette a confronto [...]

in AOV <- ip. nulla (la stessa che ho qua) => stesso risultato

* p-value <- corrisp. anch'esso al precedente


## Example: warp breaks

risposta: n rotture
esplicative:

* tipo lana <- 2lv
* tensione filo <- 3lv


```{r}
library(lattice)
data(warpbreaks)
stripplot(breaks ~ tension | wool, warpbreaks, cex=1.2, pch=16)
```

### Anova a 2 vie

Calibriamo mod analisi var a 2 vie

* Prendo come risposta la radice di breaks (sperando di ottenere res più omogeneo per i due gruppi)
* oss. la sintassi sulle var esplicative: "tension*wool" cosa vuol dire?

<- modo compatto di scrivere $tension + wool + tension/wool$ 
(effetto dei due elementi + effetto iterazione)

```{r}
breaks.aov<-aov(sqrt(breaks) ~ tension*wool, warpbreaks)
anova(breaks.aov)
```

Con summary o anova otteniamo output con f. classica (di analisi varianza). 

Cosa ci dice:

* 3 p-value <- legati a test F, simili nella logica ai test T

eg. con rif a Tension verifico se escludo o includo l'eff. di tension (mantenendo gli altri)

(<- come test T sui beta)

* riga finale dei residuals <- somma dei quad residui (sulla base di tutti i regressori) + media

* analisi gerarchica


### Stima manuale dei vari sotto-modelli

Cosa succede se faccio anova su seq. di modelli? 

Appl. a casaca dei test F

```{r}
breaks0.aov<-aov(sqrt(breaks) ~ 1, warpbreaks) # modello nullo
breaks1.aov<-aov(sqrt(breaks) ~ tension, warpbreaks) # solo tension
breaks2.aov<-aov(sqrt(breaks) ~ tension+wool, warpbreaks) # tension e wool 
# breaks.aov <- ... # mod. full di prima


anova(breaks0.aov,breaks1.aov,breaks2.aov,breaks.aov)
```

Cosa si dice?

* quali sono i mod in gioco
* serie di test F i cui p-value corrisp a quelli di prima (perché? è quello che anova ha fatto automanticamente preso dirett. il modello complesso)




```{r}
summary.lm(breaks.aov)

# confronto tra tension - tension + wool
anova(breaks0.aov,breaks.aov)
```

[...]

Perchè il anova oss. diversità dei p-value?

Anova def. test T => verifico l'effetto tension null vs positivo MA 

1) mantengo sia wool che l'iterazione
2) mantengo solo wool (l'iterazione non era ancora in gioco!)


### Costruire un mod. lineare

Considerazioni:

* prima var <- 2 lv => var binaria
* secona var <- 3 lv => 2 var binarie (con un "valore di riferimento")
* l'iterazione? dovrei introdurre 2 variabili dummy in più
* beta0 <- somma di tutti i val di riferimento

Posso prendere ogg. "breaks" (doppia classe) e lo metto in summary.lm

```{r}
summary.lm(breaks.aov)

```

* TEst F: mod nullo (solo intercetta) vs almeno 1 != 0
* intercept <- beta 0 <- media della risposta quando sono nel grup. di riferimento nei due fattori
* tensionM <- si accende la dummy riferita al fatt. tension (A,L->A,M)
=> beta0 + beta1 <- lv. medio se lana=A e passo da L a M

* tensionH <- passaggio A,M->A,H
=> beta0 + beta2 <- lv. medio se lana=A e passo da A,L a A,H

(nota: in entrambi i passaggi oss. un abbassamento)

* woolB <- dummy rif. a fattore lana, passaggio da gruppo A,L->B,L

* consideriamo ora le ultime due righe: effetto iterazione

cosa succede quando cambio sia il lv. del primo fattore che del secondo?

nel dettaglio:

* tensionM:woolB <- passaggio A,L -> B,M (tutte le var dummy attive, tranne tensionH)
* tensionH:woolB <- passaggio A,L -> B,H (in questo caso si attivano tutte le var dummy)


### ANOVA: mod. nullo vs modello full 
Ultima consid su anova: se metto a confronto modello nullo e modello full:

```{r}
anova(breaks0.aov,breaks.aov)
```

<- corrisp a test F su lm 
(tutti = 0 vs almeno 1 != 0)


# -------------------------------------------------------------------
# 19/11/2021 - Teoria


## Trattare contemporaneamente fattori e var. numeriche in lm

### Senza effetto iterazione

Modelli di analisi dewll covarianza (mod. di correlaz. lineare)

Fino adesso, appr. conveniente (x reg. categoriali): dummy

es. abbiamo un modello con una var esplicativa categoriale a 3 lv (lv di istruzione)

=> 1 intercetta  $\beta_0$ + due var. dummy $\beta_1*x_{i1} + \beta_2*x_{i2}$

$\beta_0$ <- lv. 1
$\beta_0 + \beta_1$ <- passaggio da Lv1 a Lv2
$\beta_0 + \beta_2$ <- passaggio da Lv1 a Lv3

Diciamo che si introduce un secondo regressore numerico (senza effetto iterazione)

=> aggiungo $\beta_3*x_{i3}$

Risultato finale: alla fine è come se avessi un modello per ogni combinazione, dove il coef. è lo stesso e cambia l'intercetta

### Con effetto iterazione

se avessi anche iterazione? altri due termini aggiuntivi 

Nei vari modelli non cambia più solo l'intercetta ma anche il coef angolare.

es:

x <- age, gender <- fattore binario (var dummy g), effetto iterazione

modello:

$y_i = \beta_0 + \beta_1*x_i + \beta_2*g_i + \beta_3*g_i*x_i + \epsilon_i$ 

<- riconducibile a due modelli:

* modello per solo maschi: $y = \beta_0 + \beta_1*x$ 
* modello per solo femmine: $y = (\beta_0+\beta_2) + (\beta_1 + \beta_3)*x$ 

(oss. che anche il coeff. varia)

[vedi tab. slide 69 su test F che posso fare]


### Confronto tra le rette

Analisi con var categoriali + numeriche --> insieme di modelli (paralleli o non)

se ho mod. non paralleli (effetto iterazione), ho diverse rette con diversi coefficenti

Cosa posso analizzare? potrei voler vedere se i modelli sono veramente diversi o se la diff. è accidentale.

valutare anche confronto tra modello con o senza iterazione


## Example: leaf and air temperatures 

### Rappr. grafica (con Lattice)

Rapp. grafica dei dati:

* diff. gruppi per colori
* fit sui 3 gruppi

```{r}
library(DAAG)
library(lattice)
xyplot(tempDiff ~ vapPress, # cosa mostrare
       leaftemp, 
       groups=CO2level, # dividi i gruppi per questa var
       pch=19, 
       
       # def legenda, colori, ecc.
       key=simpleKey(
          text=c('low','medium','high'), space="top", columns=3,        
          points=FALSE, col=c('blue','magenta','darkgreen'), cex=1.2), 
       
       type=c("p","r"), # fai lo scatterplot sia dei punti che delle rette di regres.
       lwd=2)
```


### Confronto tra modelli

Creiamo i vari modelli (dal più semplice a quello completo)
```{r}
leaf.lm1 <- lm(tempDiff ~ 1 , data = leaftemp)
leaf.lm2 <- lm(tempDiff ~ vapPress, data = leaftemp)
leaf.lm3 <- lm(tempDiff ~ CO2level + vapPress, data = leaftemp)
leaf.lm4 <- lm(tempDiff ~ CO2level + vapPress + vapPress:CO2level, # == vapPress*CO2level
               data = leaftemp)

```


Come prima cosa chiamiamo anova per confrontare i 3 mod. con la logica dei test T
```{r}
anova(leaf.lm1, leaf.lm2, leaf.lm3, leaf.lm4)

```


* Primi due p-value <- bassi 
=> sia vapPress che il lv. di CO2 sembrano essere influenti

* ultimo test: ipotesi di iteraz. nulla <- p-value alto 
=> l'iteraz. non sembra così significativa

E se avessi usato solo anova con full?
```{r}
anova(leaf.lm4)
```

Ultimo risultato <- equivalente
Altri <- diversi (ricordo che ora sto ver. le ipotesi: 
var. in analisi = 0, resto != 0)


Perchè non usare invece summary su lm?
```{r}
summary(leaf.lm4)
```

In questo caso faccio analisi più fine DEI SINGOLI livelli, con anova invece ho fatto un'analisi più globale delle var. esplicative.


### Confronto con AIC e BIC

```{r}
AIC(leaf.lm1, leaf.lm2, leaf.lm3, leaf.lm4)
BIC(leaf.lm1, leaf.lm2, leaf.lm3, leaf.lm4)
```

Oss. che AIC mi porta a lv. 4  e BIC a lv. 3. (come anova)

=> cosa fare? dipende da cosa cerchiamo:

* se vogliamo modello che sia semplice e robusto (es per previsioni), scelgo 3
* se invece vogliamo qualcosa più rappresentativo dei dati specifici (e non usato per previsioni) potrei scegliere il mod. 4


Diciamo che scelgo il mod. 3:

```{r}
summary(leaf.lm3)
```
Oss.

* cambia l'intercetta
* coeff angolare (vapPress, quello negativo) è sempre quello


* il passaggio da primo e secondo lv del fattore non è molto supportato dai dati (p-value alto), potrebbe essere accidentale
 
 
### Analisi diagnostica


Grafici di diagnostica:
```{r}
par(mfrow=c(2,2))
plot(leaf.lm3, which = 1, lwd=2, pch = 16, cex.caption=0.8)
plot(leaf.lm3, which = 2, lwd=2, pch = 16, cex.caption=0.8)
plot(leaf.lm3, which = 3, lwd=2, pch = 16, cex.caption=0.8)
plot(leaf.lm3, which = 4, lwd=2, pch = 16, cex.caption=0.8)
par(mfrow=c(1,1))
```
Oss:
* residui stimati e val stimati <- non ci sono particolari criticità
* qqplot <- abbastanza bene (code a parte, ma okay, li abbiamo pochi dati)
* sigma quadro errori al variare regressori <- non evidenzia particolari stranezze
* dist. di cook <- sembrano elevate, ma in realtà siamo attorno allo 0.10 (non è tanto)

### Rapp grafica del mio modello

voglio def la rel tra tempDiff e vapPress, con anche l'influenza di CO2level

* Def 3 ogg grafici (1 per gruppo)
* Li sommo

```{r}
plot1 <- xyplot(tempDiff ~ vapPress, leaftemp, groups=CO2level,pch=19,
                
# di default, R mostra solo i punti <- la retta ho voluto aggiungerla manualmente
abline = list(a=2.6849, # intercetta (oss che la ricalcolo in ogni caso)
              b=-0.8392, # coeff angolare (comune a tutte 3 le rette)
              col='blue',lwd=2),


key=simpleKey(text=c('low','medium','high'),space="top", columns=3,
points=FALSE,col=c('blue','magenta','darkgreen'),cex=1.2), lwd=2)

plot2 <- xyplot(tempDiff ~ vapPress, leaftemp, groups=CO2level,pch=19,
abline = list(a=2.6849+0.3199,b=-0.8392,col='magenta',lwd=2),
key=simpleKey(text=c('low','medium','high'),space="top", columns=3,
points=FALSE,col=c('blue','magenta','darkgreen'),cex=1.2), lwd=2)

plot3 <- xyplot(tempDiff ~ vapPress, leaftemp, groups=CO2level,pch=19,
abline = list(a=2.6849+0.7931,b=-0.8392,col='darkgreen',lwd=2),
key=simpleKey(text=c('low','medium','high'),space="top", columns=3,
points=FALSE,col=c('blue','magenta','darkgreen'),cex=1.2), lwd=2)

plot1+plot2+plot3
```


## Approfondimento paradosso di Simpson

Casi gravi, vaccini, Israele

problema della trascurazione di fattori rilevanti in un'analisi

### Dose farmaco vs prob. ricovero

* diciamo che voglio correlare la dose in mg di farmaco con la prob di ricovero.
* Raccolgo un po' di dati, li rappresento in un plot 
* facendo un fit, osservo che sembra ci sia una corr. lineare tra dose e prob. ricovero

Cosa è successo?

* oss due cluster [vedi slide] <- il fattore età (over 50 e under 50)
* provo a tenerne conto con modelli di analisi della covarianza.
* oss due rette di regressione (con pendenza ed intercetta diversa), entrambi con coef. negativo 
=> ok, osservo che in entrambi i casi la dose aiuta (anche se un po' di meno per gli over 50)


# Teoria 22/11/2021: Le var risposte non gaussiane (es. risposta bernoulliana)

## Intro

Come applicare reg. lineare per var. risposta non gaussiane?

* reg bernoullina
* reg logistica
* classe dei modelli gamma (mod additivi generaliz.)
* modelli di dispersione
* generaliz. con modellazioni non parametriche (+ flessibili)

### Direttive di generalizz

* Ci permettono di considerare fenomeni con distr. non gaussiana
* modellaz della media della var risposta <- più flessibile, si introduce una "funzione legame" tra media risposta e comb lineare delle var esplicative
* funz legame <- nella reg. lineare multipla classica è semplicemente l'identità

### esempio: progr di insegnamento

[...] (slide 75)

$ E(Y_i) = \beta_0 + \beta_1*GPA_i + \beta_2*TUCE_i +  $

[...]

## Generalized Linear Model

estensioni dei mod. di r. lineare

* utilizzo forme generali nelle quali ho una "risposta media" => tramite una link function mi ri-collego alla var risposta

In pratica, invece che una $E(Y_i) = [classico-mod - lineare] $ ho una $f(E(Y_i)) = [classico-mod - lineare] $

Risposta non gaussiana => varianza perde di senso => 
In questo contesto il metodo dei minimi quadrati (per stimare i beta) va esteso. 

Approccio: funz. di verosimiglianza, cercherò i beta che minimizzano la verosimiglianza

## Il modello di regressione logistica (analisi di dati binari)

var risposta  <- mod. di bernoulli 

(potremmo anche aggregare e considerare una distrib binomiale e/o ragionare sulle proporzioni)

si modella la MEDIA della risposta <- una probabilità, si muove nell'intervallo 0/1

=> FUNZ. LEGAME <- $f: [0,1] -> R$

$f(u) = log (u/(1-u))$

( $(u/(1-u))$ <- "odds", quote, diff. tra prob di successo e di insuccesso )

=> $p = f(u)^-1 = e^{log(odds)}/... $

NOTA: potrei avere anche altri tipi di funz. legame

* log log odds
* quantile di una normale standard (dominio quantile: [0,1])


[vedi tab slide 80]


## Studio esempio 

[...]


## La valutazione della bontà 

[...]

Ricorda dircorsto training set e test set o cross validation


## Secondo esempio (slide 85)

Dati su clienti di carte di credito, 10000 individui, variabili:

* default <- quanti sono andati in default
* student (si o no)
* income
* balance <- quanto va a spendere

Ci accorgiamo che solo il 3% è andato in default


### Balance come regressore

Da grafico slide, osservizmo che ci ha balance elevato tendenzialmente è più probabile che vada in default, income invece sembra ininfluente

previsore: $log(odds) = \beta_0 + \beta_1*BALANCE+\epsilon$

metodo max verosimiglianza => stimo i beta:

* beta 1 <- basso, ma p-value ci dice che è fortemente probabile sia diverso da 0
* oss che è positivo (si, l'incremento di balance aumenta prob default )

Usando la rel. inversa, proviamo a stimare la prob di andare in default:

[formula slide 86]

* vediamo come cambia da passaggio da 1000 a 2000 dollari (la prob aumenta parecchio)


### Consideraz. solo student
Proviamo ora a considerare l'influenza del fatt. student

$log(odds) = \beta_0 + \beta_1*STUDENT+\epsilon$

[...]

di nuovo, proviamo a verificare la prob. di studente e non studente... (anche qui si osserva una piccola diff.)

Viene fuori che gli studenti sono meno affidabili di tutti gli altri, ma è proprio così?

### Modello full (e l'effetto di confondimento)

Usiamo tutte tre le covariate

$log(odds) = \beta_0 + \beta_1*INCOME + \beta_2*BALANCE + \beta_3*STUDENT + \epsilon$

* b1 <- molto basso (con p-value alto) => scartabile
* b2 <- come prima, alta
* b3 <- negativo 

Come mai? 

I due modelli ci portano a rag. su due contesti diversi 

Qui vado a "fissare" income e balance e poi a valutare studente! => non scatta l'effetto di "confondimento". Studenti e balance tendono ad essere correlati, cioè:

* studenti tendono ad avere balance più alti
* balance alti tendono ad avere prob. default alta
* a parità di balance, gli studenti tendono ad essere più affidabili


## Es: applichiamo mod di reg logistica sull'esempio delle ammissioni (paradosso di simpson)

Variabili:

* admint <- 2 livelli
* gender <- 2 livelli
* department <- 6 livelli

Ricordiamo che apparentemente sembrasse ci fossero meno ammissioni per il genere femminile, ma che A PARITA' DI DIPARTIMENTO, la probabilità fosse circa la stessa (le F tendevano a fare domanda in dipartimenti dove - in generale - fosse più difficile entrare).

Proviamo a costruire un modello full con:

* tutti i main effect
* tutti gli iteraction effects 

var categoriale => val. di reference + variazioni delle stime attivate da var dummy 


# Regression models with discrete responses

## Example: teaching program


```{r}
program <- matrix(c(1,2.66,20,0,0,2,2.89,22,0,0,3,3.28,24,0,0,4,2.92,12,0,
                    0,5,4.00,21,0,1,6,2.86,17,0,0,7,2.76,17,0,0,8,2.87,21,
                    0,0,9,3.03,25,0,0,10,3.92,29,0,1,11,2.63,20,0,0,12,3.32,
                    23,0,0,13,3.57,23,0,0,14,3.26,25,0,1,15,3.53,26,0,0,16,
                    2.74,19,0,0,17,2.75,25,0,0,18,2.83,19,0,0,19,3.12,23,1,
                    0,20,3.16,25,1,1,21,2.06,22,1,0,22,3.62,28,1,1,23,2.89,
                    14,1,0,24,3.51,26,1,0,25,3.54,24,1,1,26,2.83,27,1,1,27,
                    3.39,17,1,1,28,2.67,24,1,0,29,3.65,21,1,1,30,4.00,23,1,
                    1,31,3.10,21,1,0,32,2.39,19,1,1), nrow=32, byrow=T)
colnames(program) <- c("OBS","GPA","TUCE","PSI","GRADE")
program <- as.data.frame(program)
```

### Tentativo di uso del modello lineare classico

Metto in rel. il val osservato di grade e il valore previsto che mi aspetto
```{r}
# modello full
mod0.lm <- lm(GRADE ~ GPA + TUCE + PSI , data = program)

plot(program$GRADE,fitted(mod0.lm),pch=19,ylim=c(-0.1,1.1),xlab="GRADE",
     ylab="Fitted values")

# (aggiungo linee di riferimento)
abline(0,0,col='red',lwd=2)
abline(1,0,col='red',lwd=2)
```

Rappr. in modo simile la stessa cosa con i box plot:

```{r}
boxplot(fitted(mod0.lm)~program$GRADE,xlab="GRADE",ylim=c(-0.1,1.1),
        ylab="Fitted values")
abline(0,0,col='red',lwd=2)
abline(1,0,col='red',lwd=2)
```

### Utilizzo di un più appropriato modello logistico

glm <- funz. più generale per i modelli di regressione, molto simile a lm

```{r}
mod.glm <- glm(GRADE ~ PSI, # stessa sintassi per diversi parametri
               data = program, 
               
               # cosa cambia? con arg. family posso specificare:
               # - che distrib mi aspetto
               # - che funz di link
               # (- che distrib dell'errore)
               # VEDI DOC
               family = binomial)
summary(mod.glm)
```

Oss: 

classe ogg output <- glm (posso usare summary) => vedo (in ordine):

* che funz ho chiamato
* che residui ho (ATTENZIONE, non centrano con [...])
* stimatori, std error e p-value associati (in questo caso ho p-value bassi ma non troppo)
* R ci ricorda che non abbiamo un param. di dispersione (sigma), è stato fissato a 1
* valutaz. circa la devianza (nel caso del modello nullo e nel caso di unico regressore)
* AIC per il modello
* quanti passi dell'algoritmo sono serviti per raggiungere il max logsimilianza / min devianza

Commento sulle stime (gia visto su slide)


Proviamo a ragionare sulla tab a doppia entrata per ricavare odds, log odds e come interpretare le stime di [...]


Costruiamo una tabella di contingenza per valori di PSI e val di GRADE:
```{r}
conttable <- table(program$PSI, program$GRADE)
conttable

```

Come prima cosa, calcoliamo i rapp. di probabilità (prob di 1/prob di 0)
```{r}
# seconda col / prima colonna
odds <- conttable[,2]/conttable[,1]
odds
```

Risultati: rapp. di probabilità nel caso di esito 0 e di esito 1


Se considero il log di odds, mi avvicino a ciò che si usa nella reg. logistica
```{r}
log(odds)
```

"Nel caso uno non partecipi, la prob. di avere ... è inferiore rispetto a quello di averlo"

NOTA:
* come mi aspettavo, il primo log(odds) coincide con $\beta_0$ stimato

E per il secondo? 

$log(odds) = \beta_0 + \beta_1$

$log(odds) = log(PSI_1) - log(PSI_0) = log(odds) = log(PSI_1/PSI_0)$ <- RIVEDI!!!!


(-1.6 + 1.8 = 0.2) 



```{r}
OR <- odds[2]/odds[1]
OR
```



```{r}
mod.glm$coef[1]
mod.glm$coef[1]+mod.glm$coef[2]
mod.glm$coef[2]
log(OR)

```

```{r}
mod.glm.all <- glm(GRADE ~ PSI + TUCE + GPA, family = binomial, data = program)
summary(mod.glm.all)
```

AIC è più piccolo => introdurre questa nuova var. nel modello ha introdotto un miglioramento


Plot: 
* ascisse <- val osservato effettivo
* ordinate <- val stimati medi (probabilità esito positivo)

```{r}
plot(program$GRADE,mod.glm.all$fitted.values,pch=19,ylim=c(-0.1,1.1),xlab="GRADE",
     ylab="Fitted values")
abline(0,0,col='red',lwd=2)
abline(1,0,col='red',lwd=2)
# the following commands give the same output
# mod.glm.all$fitted.values
# exp(predict(mod.glm.all))/(1+exp(predict(mod.glm.all)))

# predict <- si usa anche per ogg. di classe ml

# senza spec. di new data ho come res di default NON è la stima di prob di successo ma la stime del previsore lineare => stima log (odds) 
# => prendo la stima e la trasformo con funzione inversa del log
```

Voglio mettere a confronto i val osservati di grade e i val stimati:

* soggetto per soggetto effettuo (manualmente) una previsione
* metto nella tabella

```{r}
pred <- as.numeric(
  exp(predict(mod.glm.all)) / 
    (1+exp(predict(mod.glm.all))) >0.5 # prevedo 1 quando >0.5, altrimenti 0
  )

table(pred,program$GRADE)

```

Diagonale \\ <- previsioni corrette
Diagonale // <- previsioni errate


```{r}
(18+8)/        # previsioni corrette
  (18+3+3+8)   # tutte le previsioni
```

NOTA: ricorda che questa valutaz è ottimistica perchè sto valutando il modello sugli stessi dati su cui l'ho calibrato 

=> potrei provare a fare una cross validation:
```{r}
library(DAAG)
set.seed(11)
CVbinary(mod.glm.all) # cross. validation di un modello
```

* Fold <- sottogrupi usati per fare il fit
* Acc. predittiva senza cv
* Acc. predittiva con cv

E per fare una CV con tanti fold quanti i dati?

```{r}
CVbinary(mod.glm.all, nfolds=length(program$GRADE))

```

NOTA: la CV non mi serve tanto per valutare la bontà di un singolo modello fittato MA
a comparare diversi modelli (che cambiano proprio per struttura)

(e lo faccio andando )

## Example: credit card ##


* Fenomeno d'interesse: default (si/no)

* Potenziali var esplicative:
 ** Saldo mensile (numerica)
 ** income(numerica)
 ** studente (si/no)

```{r}
library(ISLR)
library(lattice)
attach(Default)
xyplot(income ~ balance, 
       groups=default,# raggr (e coloro) in base a chi è andato in default e chi no
       data=Default,auto.key=list(columns=2))


xyplot(income ~ balance, 
       groups=student, # potrei anche raggr per student
       data=Default,auto.key=list(columns=2))
```


```{r}
par(mfrow=c(1,2))
boxplot(balance~default, data=Default,xlab="default",ylab="balance",
        col=c("deepskyblue","magenta"))     
boxplot(income~default, data=Default,xlab="default",ylab="income",
        col=c("deepskyblue","magenta"))
par(mfrow=c(1,1))
```


```{r}
credit1<-glm(default~balance,family="binomial", data=Default)
summary(credit1)
```

### Utilizzo di predict per effettuare la previsione

Intercetta <- negativa
balance <- nonostante sia basso, è comunque positivo e != 0 (vedi p-value molto basso)

NOTA: ricordo che il modello lavora comunque sulla scala della trasformata

Con la funz. predict voglio vedere cosa succede sulla scala della risposta:
```{r}
predict(credit1, 
        
        # in questo caso effettuiamo la previsione su due valori nuovi
        data.frame(balance=c(1000,2000)) ,
        
        # previsione sulla scala della risp. (così ottengo dirett. la probabilità)
        type = c("response"))

```

Oss. una variazione considerevole nella variaz. della risposta per i val. 1000 e 2000

### Studend ed "effetto di confounding"

Creiamo un modello che va a considerare la var student: 

```{r}

# (in questo caso aggiungo un vett numerico e lo imposto a 1 quando student == "Yes")
Default$studentD<-0
Default$studentD[Default$student=="Yes"]<-1

# (modello)
credit2<-glm(default~studentD,family="binomial", data=Default)
summary(credit2)

# the same result can be obtained with the folowing command, using the variable student
# credit2<-glm(default~student,family="binomial", data=Default) 
```

Come abbiamo visto, considerando solo Student (ignorando tutto il resto) oss. che cresce la prob. di default.

Quanto cresce? come prima facciamo la previsione
```{r}
predict(credit2,data.frame(studentD=c(1,0)),type = c("response"))
```

Oss. aumento di 1% quando studente (da p-value e considerato "significativo")


PRoviamo a considerare il modello completo:
```{r}
credit3<-glm(default~income+balance+studentD,family="binomial", data=Default)
summary(credit3)
```
Oss: 

* income è irrilevante
* studentD, nel mod. completo, risulta negativo 

studenti -> lv alto di balance -> più alta prob di default

Nel complesso, a parità di balance, gli student si comportano anche un po' meglio dei non-student

## Example: UCB admissions

Ammissioni di M e F:

Dataset <- array a 3 dimensioni UCBAdmissions

Cosa abbiamo? freq. congiunte con rif alle 3 var categoriali

* genere
* ammesso o no
* dipartimento

Costruiamo una f. che fa il rapporto tra gli odds 
(voglio confrontare gli odds delle ammissioni tra maschi e femmine)

### Il problema: 

```{r}
odds.ratio<-function(x){(x[1,1]*x[2,2])/(x[1,2]*x[2,1])}

# applico la funzione a tutte 6 le tabelle (6 dipartimenti)
apply(UCBAdmissions,3,odds.ratio)
```

<- 6 odds ratio per ogni dipartimento. Cosa ci dice:

* A,B,D,F <- a favore per donne
* C, E <- a favore degli uomini
* A è l'unico dove la quota è molto forte


Ora, vogliamo marginalmente calcolare la tab. riassuntiva: 
```{r}
margin.table(UCBAdmissions,c(2,1))
```

Qui, senza tenere conto di cosa succ. nei dipartimenti, i maschi sembrano favoriti

```{r}
odds.ratio(margin.table(UCBAdmissions,c(2,1)))
```

(si vede anche nell'odds ratio, molto alto)

### Andiamo ad approfondire...

Costruiamo un dataset dove, per ogni dipartimento (e sesso), conto ammessi e rifiutati.

Calcolo anche il numero totale di appllications e la f. di ammessi (p).

```{r}
UCB <- as.data.frame.table(UCBAdmissions["Admitted", , ])
names(UCB)[3] <- "admit"
UCB$reject <- as.data.frame.table(UCBAdmissions["Rejected", , ])$Freq
UCB$Gender <- relevel(UCB$Gender, ref="Male")
UCB$total <- UCB$admit + UCB$reject
UCB$p <- UCB$admit/UCB$total
UCB
```

NOTA: che dataset strano.

Dati grezzi <- bernoulliano, mi sarei aspettato un dataset con 1 riga per ogni singola application

In questo caso, però posso permettermi di mantenere la forma compatta. Come? ù

Binomiale <- somma esiti bernoulliane

```{r}
UCB.glm1 <- glm(p ~ Dept+Gender, # l'ogg. formula viene però espresso in modo un po' particolare
                family=binomial, # famiglia (la stessa)
                data=UCB, 
                weight=total # devo aggiungere il weight per introdurre l'info sul totale
                )


summary(UCB.glm1)

# the same result can be obtained with the command
# alternativamente mettevo come var d'interesse annesi e rifiutati
# UCB.glm1 <- glm(cbind(admit, reject) ~ Dept+Gender, family=binomial, data=UCB)
```

Cosa ottengo?

* intercetta <- maschi che far rif. al dip A
* DeptB ... DeptF <- tutte le variaz. osservate quando cambio dipartimento
* [...]
* e GenderFemale? mi sta ad indicare come varia il log odds se prendo gender F per qualsiasi dipartimento (non ho l'effetto di prima perché ho fatto la consideraz. su ogni dipartimento)

Come valuto su questi dati l'odds ratio maschi femmina? 


Odds effetto di essere F:
```{r}
exp(UCB.glm1$coefficients[7])
```

Con inverso, ottengo odds effetto di essere M:
```{r}
1/exp(UCB.glm1$coefficients[7])
```


### Anova

```{r}
anova(UCB.glm1, 
      test="Chisq" # nel caso della reg. logistica, si usa test Chi quadro
      )
```


"rispetto a mod nullo..."

* se considero dipartimento, spiega molto
* se considero anche gender, non ho un miglioramento molto significativo! (p. value alto, come il beta di gender=Female)


### Stimma del modello di prima invertendo Gender e dept

```{r}
UCB.glm2 <- glm(p ~ Gender+Dept, family=binomial, data=UCB, weight=total)
summary(UCB.glm2)
```

Il ris. è lo stesso.

Ma cosa succede se applico anova?
```{r}
anova(UCB.glm2, test="Chisq")
```

Qui, sto considerando un modello SOLO con gender (in confr. al modello nullo)

=> ottengo una "spiegazione rilevante" (ATTENZIONE: non completa, difatti anche aggiungere dept aggiunge bontà)

### Ultimo step: modello completo (anche eff. iterazione)

Costr. modello completo:
```{r}
UCB.glm <- glm(p ~ Dept*Gender, family=binomial, data=UCB, weight=total)
summary(UCB.glm)
```

Oss.
* AIC è il più basso (migliore rispetto a tutti gli altri, anche di molto)
* reference <- maschio, dip A
* DeptB ... F <- come cambia se un maschio da rif a dip ...
* GenderFemale <- femmina MA su dip A
* [...] <- femmina ma dip B

(NOTA: calcolo di beta di Femmina dip B 
<- devo considerare: intercetta + DeptB + GenderFemale + DeptB:GenderFemale)

Per chiudere, applichiamo anova al mod. completo:
```{r}
anova(UCB.glm, test="Chisq")
```

* rispetto a mod nullo, considerare il dip è una buona cosa
* rispetto a solo dip, gender NON è rilevante
* SI OSSERVA CHE L'EFFETTO ITERAZIONE E' MOLTO SIGNIFICATIVO

=> Gender non è rilevante globalmente ("mediamente") MA vale all'interno dei dipartimenti





