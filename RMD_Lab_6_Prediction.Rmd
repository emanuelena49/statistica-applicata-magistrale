---
title: "Lab 6 - Predictive and classification methods"
author: "Emanuele Lena - 142411@uniud"
date: "29/11/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Teoria 29/11/2021

Previsione <- utilizzare i modelli (che abbiamo visto) per 


NOTE: 

* previsione fatta per essere smentita
* a lungo andare, l'utlizzo di metodi preditt
* in certi contesti, 


## Fallacia delle previsioni

Perché fare una previsione è difficile? perchè una previsione può non essere corretta?

p errore <- non sarà mai zero (variabilità non eliminabile)

1) Variabilità di fondo non eliminabile

  * complessità situazioni
  * intervento umano
  * situazioni inaspettate


2) Esistono proc. predittive più o meno performanti. Da che cosa dipende?

  * processato i dati in modo corretto
  * scelto e validato un modello in modo opportuno
  * estrapolaz. non giustificata (quando estrapolo informazioni da contesti dove non ho effettivamente osservato dati - es. uso un modello per effettuare previsioni "fuori dal range dei dati")
  * overfitting
  
## Inferenza vs previsione

$Y=f(X) + \epsilon$ <- tipico modello di regressione

$f$ <- mi rappresenta quelli che sono i legami tra X e Y

* p. di inferenza <- comprendere il legame tra X e Y (la relazione), come cambia Y in corrispondenza ad X. Mi interessa studiare la struttura di f

* p. di predizione <- il focus si sposta su Y. Non mi interessa tanto più "comprendere e studiare" $f$ quanto usarlo (come scatola nera) per effettuare prevsioni

## Come misurare la bontà di un fit?

supponiamo y numerico

ins. di osservazioni ("di training") per le y e le x

Stimiamo la funzione f (cappello) <- modello

Dato il modelo, posso usarlo per prevedere dei valori y (cappello)

Come valuto bontà previsione?

1) calcolo dell'errore quadratico medio sul training set (training MSE)) [formula]

NOTA: molto simile a sigma^2, solo che qui non consideriamo 

NOTA 2: il fatto che abbbiamo usato gli stessi dati sia per valutaz. che per test, ci porta ad avere una stima ottimistica

2) => consideriamo di usare un'ulteriore dataset di (m) osservazioni (diverso da quello del training). Applichiamo il calcolo dell'MSE su questi nuovi dati (test MSE)


### Complessità del modello vs flessibilità (overfitting)

Modello complesso <- spesso è il migliore per la RAPPRESENTAZIONE dei dati, non è detto che lo sia per la previsione.

Introdurre complessità <- ok fino ad un certo punto, dopo si cade nell'overfitting (il modello "cattura" anche la variabilità casuale dei dati di addestramento)

MSE <- cala con l'aumentare della flessibilità ma fino ad un certo punto, dopo:

* consinua a calare nel training MSE (ovviamente)
* sale nel test MSE (il modello è caduto in overfitting)

### Approfondimento su test MSE

Cosa facciamo quando calc. l'MSE?

Stiamo facendo una stima (con il valore medio) dei quadrati della diff. tra valore atteso e val stimato

Si oss che si può scomporre in 3 componenti:

1) variabilità del previsore
2) distorsore del previsore
3) variabilità dell del termine d'errore (varianza errore) 


la var dell'errore non si può azzerare, 

i primi due elementi invece sono in rel tra di loro (e bisogna trovare un trade-off).

[...]

## Y non numerica

Tipi di p. di previsione:

* p. con Y numerica <- p. di regressione
* p. con Y categoriale <- p. di classificazione




## Install the packages "MASS", "lattice", "ISLR", "boot", "ElemStatLearn", "class", "crossval", "verification", "RWeka"  ##

```{r}
# install.packages("MASS")
# install.packages("lattice")
# install.packages("ISLR")
# install.packages("boot")
# install.packages("ElemStatLearn")
# install.packages("class")
# install.packages("crossval")
# install.packages("verification")
# install.packages("RWeka")
```



## Example: advertising data

Data set given in the book "An Introduction to Statistical Learning: with Applications in R" 
by G. James, D. Witten, T. Hastie and R. Tibshirani and saved in the .csv file "Advertising.csv"
in the working directory

```{r}
Advertising<-read.csv(file="data/Advertising.csv",header=TRUE)[,-1]
str(Advertising)
```


### scatterplot matrix

```{r}
# creo una funzione che innserisca nella diagonale un istogramma
panel.hist <- function(x, 
                       ... # "ellipsis" = qui ci sono degli arg. che dipendono da situaz. a situaz.
                       )
{
    usr <- par("usr")
    on.exit(par(usr))
    par(usr = c(usr[1:2], 0, 1.5) )
    h <- hist(x, plot = FALSE)
    breaks <- h$breaks
    nB <- length(breaks)
    y <- h$counts
    y <- y/max(y)
    
    # disegna seq di rettangoli fornendo: 
    # coord. angolo sup dx e inf. sn
    rect(breaks[-nB], 0, breaks[-1], y, col = "cyan", ...) 
}

# creaz. del pannello di plot a coppie (specificando l'istogramma nella diagonale)
pairs(Advertising, panel = panel.smooth, cex = 1.5, pch = 1, bg = "light blue",
      diag.panel = panel.hist, cex.labels = 2, font.labels = 2)
```


### first multiple linear regression model

Creiamo un modello che coinvolge tutti i regr.
```{r}
mod.adv <- lm(Sales~TV+Radio+Newspaper, Advertising)
summary(mod.adv)
summary(mod.adv)$sigma^2 # estimated error variance
AIC(mod.adv) # AIC criterion

par(mfrow=c(2,2), pty="s", mar=c(3,2,3,2))
plot(mod.adv)
par(mfrow=c(1,1))
```

Oss: 

* le diagnostiche non sono un gran che (variabilità non costante)



### second multiple linear regreession model

```{r}
mod.adv1 <- lm(Sales~TV+Radio+I(TV^2)+TV:Radio, Advertising)
summary(mod.adv1)
summary(mod.adv1)$sigma^2 # estimated error variance
AIC(mod.adv1) # AIC criterion

par(mfrow=c(2,2), pty="s", mar=c(3,2,3,2))
plot(mod.adv1)
par(mfrow=c(1,1))
```

Oss: 

* secondo tutte stime, il secondo mod è migliore
* le diagnostiche hanno ancora qualche val anomalo ma ok.


### confidence and prediction intervals

Effettuiamo una stima ed una previsione (oss. la diff tra gli intervalli)

```{r}
intc <- predict(mod.adv1, newdata=data.frame(TV=100,Radio=20), 
                interval="confidence")
intc
```



```{r}
intp <- predict(mod.adv1, newdata=data.frame(TV=100,Radio=20), 
                interval="prediction")
intp
```

## Example: automobile bodily injury claims

Data set given in the book "Regression Modeling with Actuarial and Financial Applications" 
by E.W. Frees and saved in the .csv file "AutoBI.csv" in the working directory


```{r}
autobi=read.csv(file="data/AutoBI.csv",header=TRUE)[,-1]
str(autobi)
```

OSs. R non interpreta qualcosa come fattori => glielo dico manualmente:
```{r}
autobi$ATTORNEY <- factor(autobi$ATTORNEY)
levels(autobi$ATTORNEY) <- c("yes","no")
autobi$CLMSEX <- factor(autobi$CLMSEX)
levels(autobi$CLMSEX) <- c("male","female")
autobi$MARITAL <- factor(autobi$MARITAL)
levels(autobi$MARITAL) <- c("M","S","W","D")
autobi$CLMINSUR <- factor(autobi$CLMINSUR)
levels(autobi$CLMINSUR) <- c("yes","no")
autobi$SEATBELT <- factor(autobi$SEATBELT)
levels(autobi$SEATBELT) <- c("yes","no")

# + def delle classi di età 
autobi$AGECLASS <- cut(autobi$CLMAGE, breaks=c(0,18,26,36,47,95))
levels(autobi$AGECLASS) <- c("1","2","3","4","5")

str(autobi)
```

Ora ho il ds diviso per fattori:
```{r}
summary(autobi)
```


### exploratory data analysis

#### Rappr. della densità (istogramma)
```{r}
library(MASS)
par(mfrow=c(1,3), pty="s")

# hist.scott <- 
hist.scott(autobi$LOSS, main="LOSS", xlab="", col="lightblue")
rug(autobi$LOSS, col="lightblue") # inserisci su ascisse pos. dati osservati

hist.scott(autobi$LOSS[-which.max(autobi$LOSS)], main="LOSS (without max value)",
           xlab="",col="lightblue")
rug(autobi$LOSS[-which.max(autobi$LOSS)],col="lightblue")


hist.scott(log(autobi$LOSS), main="log(LOSS)",xlab="",col="lightblue")
rug(log(autobi$LOSS),col="lightblue")
```

Oss:

* c'è una forte assimmetria nei dati (no gauss.)
* oss. un valore fortemente anomalo (proviamo a rimuoverlo)
* Anche rimuovendolo, mi rendo conto che comunque osservo un andamento parecchio assimmetrico (e non gaussiano)
* => proviamo a passare al log(LOSS) <- in questo caso, osservo già qualcosa di migliore (probabilmente non è gaussiano, è qualcosa tipo T di student)


#### Rappr. (in vari modi) delle mie variabili esplicative

* grafici a barre per categoriali
* hist per v. numeriche

```{r}

# def struttura di "come stampo i grafici" (griglia 4x2)
par(cex.axis=1.3,cex.lab=1.3, mar=c(5,3,1.5,1))
layout(matrix(c(1:7,7),byrow=TRUE,nrow=2))

# g. a barre per per le categoriali
ind <- c(1,2,3,4,5,8)
for (i in 1:6)
   barplot(table(autobi[,ind[i]]) , xlab=names(autobi)[ind[i]], 
                col="lightblue")

# istogramma per la numerica (l'età grezza)
hist(autobi$CLMAGE,col="lightgreen",xlab="CLMAGE",ylab="",main="",border="white")
```


#### Rel risposta vs esplicative 

Ora proviamo a mettere in rel. la var. risposta con le esplicative visualizzando un grafico a barre per le oss. relative ad ogni lv. di ogni fattore:
```{r}
library(lattice)
bwplot(log(LOSS)~ ATTORNEY + CLMSEX + MARITAL + CLMINSUR + SEATBELT + AGECLASS,
       data=autobi, ylab="log(LOSS)", outer = TRUE, scales = list(x = "free"),
       xlab="", layout=c(3,2), main="", aspect="fill")  
```
  
La vis. grafica non è la migliore, proviamo a ripetere un appr. simile a prima:

```{r}
par(cex.axis=1.3,cex.lab=1.3, mar=c(5,3,1.5,1))
layout(matrix(c(1:7,7),byrow=TRUE,nrow=2))
ind <- c(1,2,3,4,5,8)
for (i in 1:6)
   boxplot(log(autobi$LOSS)~ autobi[,ind[i]] , xlab=names(autobi)[ind[i]], 
                col="lightblue")
plot(autobi$CLMAGE,log(autobi$LOSS),xlab="CLMAGE",ylab="",main="",pch=20)
```

OSs

* essere rapp da avvocato determina shift chiaro
* no diff genere
* influenza strana per stato civile
* la cintura di sicurezza ha una chiara influenza
* età (ragg in classi) è sembra rilevante

Confrontiamo le f. di densità per quando sono rapp da avvocato e quando no:
```{r}
library(lattice)
densityplot(~log(LOSS), group=ATTORNEY, data=autobi, lwd=2,
       xlab="log(LOSS)",  plot.points=FALSE, auto.key=list(columns=2))
```

Oss:

* ho usato metodo del nucleo
* ho usato densityplot di LATTICE per ottenere res migliore graficamente
* oss che il fatto che ci sia stato un avvocato, sposta la curva di densità a dx e la rende anche più stretta

### first multiple linear regression model for log(LOSS)

Costruiamo mod di regressione di tipo gaussiano. Prendiamo come var. ATTORNEY (rapp da avvocato o no) + la classe d'età

```{r}
mod <- lm( log(LOSS) ~ ATTORNEY + CLMAGE, autobi)
summary(mod)
```

OSS:
* 
* ...
* sono state escluse 189 oss per mancanza di una var o dell'altra



Facciamo diagnostica:

```{r}
par(mfrow=c(2,2), pty="s", mar=c(3,2,3,2))
plot(mod)
par(mfrow=c(1,1))
```

* 
* qqplot non è male
* varianza dell'errore è abbastanza costante
* per le anomalie, si notano alcuni dati che andrebbero ri-valutati


### prediction intervals

Proviamo ad effettuare qualche previsione:

```{r}
ci_yes<-predict(mod, newdata=data.frame(ATTORNEY="yes",CLMAGE=30,SEATBELT="yes"), 
                interval="confidence")
ci_yes # age=30 and attorney=yes, log scale
exp(ci_yes) # age=30 and attorney=yes, original scale



ci_no<-predict(mod, newdata=data.frame(ATTORNEY="no",CLMAGE=30,SEATBELT="yes"), 
               interval="confidence")
ci_no # age=30 and attorney=no, log scale
exp(ci_no) # age=30 and attorney=no, original scale
```

NOTA: la copertura della trasformata non è proprio del 95%. 

Le procedure di cambio scala non sono proprio "invarianti" => ho giusto un'approssimazione. 


### second multiple linear regression model for log(LOSS)

Miglioriamo il modello:
* agg quadrato età
* agg cintura di sicurezza

```{r}
mod2s <- lm(log(LOSS) ~ATTORNEY + CLMAGE + I(CLMAGE^2) + SEATBELT, 
            data=autobi,  subset = complete.cases(autobi))
summary(mod2s)
AIC(mod2s)
AIC(mod)
```


### cross-validation and test MSE

boot <- ...

```{r}
library(boot)

autobi_complete<-autobi[complete.cases(autobi),] # data frame with complete observations

mod_bis <- glm(log(LOSS) ~ATTORNEY + CLMAGE, data=autobi_complete)
mod2s_bis <- glm(log(LOSS) ~ATTORNEY + CLMAGE + I(CLMAGE^2) + SEATBELT, 
                 data=autobi_complete)

```

```{r}
sum(resid(mod_bis)^2)/length(autobi_complete[,1]) # training MSE for mod_bis
cv.glm(autobi_complete,mod_bis)$delta[2] # cv estimate test MSE for mod_bis
```

```{r}
sum(resid(mod2s_bis)^2)/length(autobi_complete[,1]) # training MSE for mod2s_bis
cv.glm(autobi_complete,mod2s_bis)$delta[2] # cv estimate test MSE for mod2s_bis
```


### confidence and prediction intervals

```{r}
intc <- predict(mod2s, newdata=data.frame(ATTORNEY="yes",CLMAGE=30,
                SEATBELT="yes"), interval="confidence")
intc # log scale
exp(intc) # original scale
intp <- predict(mod2s, newdata=data.frame(ATTORNEY="yes",CLMAGE=30,
                SEATBELT="yes"), interval="prediction")
intp # log scale
exp(intp) # original scale
```


```{r}
intp <- predict(mod2s, newdata=data.frame(ATTORNEY="yes",CLMAGE=30,
                SEATBELT="yes"), interval="prediction", 
                
                # posso indicare la stima di sigma^2 
                # (in questo caso uso il val del testMSE, che è certamente 
                # più grande => l'intervallo di previsione sarà più ampio)
                pred.var=1.4122 
                )
intp # log scale
exp(intp) # original scale
```

Vediamo come cambia l'intervallo di previsione cambiando l'età: 
```{r}
age <- seq(18, 80, l=20) # prendo 20 età tra ...

matout <- matrix(0, nrow=20, ncol=3)
for(i in 1:length(age))
matout[i,] <- predict(mod2s, newdata=data.frame(ATTORNEY="yes",CLMAGE=age[i],
                SEATBELT="yes"), interval="prediction")
matout
```


```{r}
par(mfrow=c(1,2), pty="s")

# valori predetti sia come linea che come punti
plot(age, matout[,1], pch=16, type="l", ylim=range(matout), xlab="CLMAGE", 
     ylab="log(LOSS)")
points(age, matout[,1], pch=16)

# bande di previsione inferiore e superiore
lines(age, matout[,2], col=2, lwd=2)
lines(age, matout[,3], col=2, lwd=2)

# (stessa op su scala esponenziale)
abline(v=30)
plot(age, exp(matout[,1]), pch=16, type="l", ylim=range(exp(matout)), 
     xlab="CLMAGE", ylab="LOSS")
points(age, exp(matout[,1]), pch=16)
lines(age, exp(matout[,2]), col=2, lwd=2)
lines(age, exp(matout[,3]), col=2, lwd=2)
abline(v=30)



par(mfrow=c(1,1))

```

# Teoria 01/12/2021 : Previsioni su var categoriali

"procedure di classificazione" = "previsioni su var. categoriali" (a 2 o + lv)

In cosa si concretizza? 

Stima della prob. oss. appartenga ad una categoria piuttosto che un'altra
=> stimo prob eventi, in base a p. stimate scelgo

## Il tasso d'errore

c'è un analogo dell'MSE per le categoriali? 

Si posso effettuare previsioni e confrontarle con i val. osservati 
=> posso calcolare un tasso d'errore (e pormi l'obb. di minimizzarlo)

$trainingER = 1/n *\sum^n_{i=1} I(y_i!=y_i cappello)$

$I(y_i!=y_i cappello) = 1$ se la cond. è vera, altrimenti 0

Analogamente, posso calcolare lo stesso tasso per il test set: $testER = ... $

ER <- stimatore dell'errore di predizione 

$E[I(Y_0!=Y_0capp.)]=P(Y_0!=Y_0capp.)$ (media tasso errore = prob. errore)

val. atteso <- probabilità  di avere una predizione diversa da quella attesa

Il classificatore deve minimizzare l'err. di prediz. => il tasso d'erroe

## Il classificatore di Bayes 

miglior classificatore (teorico) possibile. Assegna ad un'osserv. ($x_0$)

* la classe 1  se la prob. condizionale $P(Y_0 = 1 | X_0 = x_0) > P(Y_0 = 0 | X_0 = x_0)$
* la classe 0 altrimenti

problema: la prob. $P(Y_0 = 1 | X_0 = x_0)$ è solo un valore teorico, non lo conosciamo 

=> serve stimatore, dalla bondtà dello stimatore dipende la bontà del classificatore

### Il Bayes decision bnoundaries

Prendiamo un set di osservazioni, costituito da:

* due var. numeriche $X_1, X_2$ (predittori)
* una var. categoriale (2 lv) <- la var. per la quale sono interessato a classificare

Facciamo un plot dei punti su un piano, coloriamo di rosso e nero i pt. a seconda della classe.

conosco la prob. condizionale => per ogni pt del piano posso determinate una prob di appartenza ad una cls piuttosto che l'altra

Bayes decision boundary <- l'insieme dei punti del piano dove questa prob. è =0.5

(slide 38)

## Utilizzo del regressore logistico per la classificaz.  

mod. di reg. logistico <- può essere uno stimatore della p. condizionata di appart. ad una classe

$P(Y=1|X=x) = exp(x_i^T*\beta)/(1 + exp(x_i^T*\beta))$

Decision boundary stimato <- retta $ x_i^T*\beta=0 $

Efficace fintanto che il d.b. è vagamente lineare

## Linear Discrimination Analysis (LDA)

utilizzo diretto della reg. lineare su risposte categoriale <- eff. simile a mod logistico (difetto: può dare prob. fuori dall'int $[0,1]$)

Potrei però pensare ad un uso "indiretto":
* parto dal modello del prediittore
* ottengo la prob. condizionale di interesse $ P(Y_i=y_i|X_i=x_i)$ con t. di bayes

Esempio:

* var. risposta Y, $S>=2$ livelli (non ordinati) $y_1, ..., y_s$
* X <- vettore di predittori continui (dim p)
* $f_s(x)=f(x|Y=y_s)$ <- funzione di densità dei predittori lineari per la cat. s (la conosco)
* $\pi_s = P(T=y_s)$ <- prob. marginale per la cat. s

(per la categoria s, posso computare un certo val stimato - la media della gaussiana - usando un mod. lineare costruito su X)

Per t. di Bayes, posso fare calcolo inverso:

$ P(Y=y_s|X=x) = \pi_s*f_s(x) / \sum_{r=1}^S(\pi_r*f_r(x)) $


### LDA concreto

Problema: non conosco le funzioni di densità $f_s(x)$

<- usiamo una distrib. gaussiana $N(\mu_s, \sigma^2)$

* media <- stimo con media delle oss. $x_i$ appartenenti alla classe $y_s$ (varia di classe in classe)
* varianza <- varianza complessiva di x (calcolata usando la media specifica per ongi classe e S gradi di libertà)

E le $\pi_s$? $n_s/n$ (percent. di oss. appartenenti alla classe s del training set)

=> LDA <- assegna la classe per la quale questo valore è più alto: 

$\delta_s(x) = x * \mu_s/\sigma^2-\mu_s^2/(2\sigma^2)+log(\pi_s)$

(ovviamente per $\mu_s, \sigma^2, \pi_s$ si usano i risp. stimatori cappello)

NOTE VARIE: 
* Se per due categorie ho lo stesso valore? uso dec. boundary lineare classico
* LDA e reg. logistica <- simili, ma LDA è più "stabile"
* LDA assume la continutià dei predittori (con stessa media per ogni classe e varianza comune) => non va bene se anche i predittori sono categoriali

* per set grandi (e se le assunzioni sono tutte soddisfatte) LDA approssima bene lo la regola di Bayes teorica

### Quadratic Discriminant Analysis (QDA)

se rilassiamo ipotesi di varianza costante, possiamo pensare a determinare d.b. quadratici.

Non è molto usata e richiede set molto grandi.

## K-Nearest Neighbors (kNN)

Procedura alternativa, moolto semplice e molto usata. 

Data un'oss $x_0$, per la classif. osservo i $k>0$ punti (del training set) più vicini. 

Stimo $ P(Y_i=y_i|X_i=x_i)$ con la media di $y_i$ per i punti (con i vari $y_i$ che valgono 1 se i punti appartengono alla classe, altrimenti 0).

Ovviamente, scelta di k influisce pesantemente:
* $k=1$ <- mi baso solo sul punto più vicino
* ...
* $k=n$ <- uso direttamente la probabilità marginale ingorando i regressori

In base alla scelta di k ottengo decision boundaries più o meno "fittati" sui dati. 

Es. $k=10$ <- classif. elastica, $k=50$ <- classif. più rigida, più vicina al db lineare della reg. logistica

NOTE:
* kNN efficace quando ho predit. numerici e il db è chiaramente non lineare
* per utilizzare kNN con predit categoriali, devo "definire il concetto di vicinanza"
* potrei sperimentare diversi k e scegliere quello che minimizza il rateo di classif errate (in base alle esigenze [...]) <- per effettuare la scelta posso sia applicare cross-validation che usare dati di test a parte (per evitare di essere troppo ottimistici)

* k piccoli <- previsioni molto suscettibili a "rumore dei dati"

* (bias-variance trade off [...])

* potenzialmente, potrei usare kNN anche per problemi di regressione (quando voglio approssimare una risposta continua)


# Teoria 6/12

## La matrice di confusione

matr. 2x2 che rappr. tutte le situaz. verificabili in una previsione (associa val previsto e valore osservato).

In particolare, per var. risp. binaria:

* True Positive e True Negative <- (previsioni corrette)
* Falsi Negativi <- prevedo un positivo ma oss. un negativo
* Falsi Positivi <- prevedo un positivo ma oss. un negativo

Per ongiuno di questi casi, avrò una frequenza.

Sulle f., posso calcolare un pario di valori utili a valutare un classificatore:

* la (semplice) percentuale di risposte corrette [formule]

  * specificità
  * sensibilità 

(valori più specifici)

* valore predittivo positivo <- prob. che l'oss sia realmente 1 quando il classif. da 1
* valore predittivo negativo <- [...]

* log del rapp. di odds <- [...]

Di nuovo, tutte queste metriche saranno troppo positive se uso lo stesso set per training e test.

## La curva ROC

scelta <- in base a prob. stimata, soglia unica a 0.5

E se definissi più soglie? 

Come confrontare più classif. introducendo flessibilità per la soglia.

Strumento grafico: Curva ROC
<- valutaz. dell'a bontà del classi'efficacia del classif. al variare della scelta della scoglia.

Mette in relazione true positive rate e false positive rate

[vedi slide 54]

Ottimo <- per val X vicini a 0 e val y vicini a 1

Per ogni soglia, ottengo un TPR e un FPR => ottengo un punto su grafico (=> formo curva ROC)

Poi potrei valutare anche l'area sotto la curva AUC [...]


[...]

uso curva per confrontare classif. diversi (per qualunque val soglia)

## Note di chiusura

Esistono tanti classificatori:

* reg. logistica
* classif. bayesiano (teorico)
* LDA e QDA
* kNN

* alberi di classif. 
* ...

* combinaz. di classif semplici (interessante ambito di ricerca)
  * foreste casuali (+ alberi)
  * bootstrap aggregator
  * alg. di boosting (ripeti + volte l'uso del class. forzando a valutare casi classif. male)
  
* support vector machines

## Example: credit scoring ###

Data set given in the book "Regression: Models, Methods and Applications" by L. Fahrmeir, Th. Kneib, 
S. Lang and B. Marx and saved in the .txt file "Scoring.txt" in the working directory

PREMESSA: valutaz. "solvibilità" di una persona che chiede un prestito (sarà in grado di ritornare quanto ricevuto oppure no?). => 

var interessa categoriale <- pagherà o non pagherà il prestito? (1 o 0)

var (ipoteticamente) esplicative (categoriali?):

* acc1 <- presenza cc o no
* acc2 <- comportamento positivo o negativo nel conto

<- raggruppati in categoriale a 3 lv

* moral
* intuse
* ...


```{r}
Scoring <- read.table(file="data/Scoring.txt",header=TRUE)
# acc1=1 (no runnig account) acc1=0 (good or bad running account)
# acc2=1 (good running account) acc2=0 (no or bad running account)
Scoring$account <- 1 - Scoring$acc1 + Scoring$acc2
Scoring$account <- factor(Scoring$account)
Scoring$moral <- factor(Scoring$moral)
Scoring$intuse <- factor(Scoring$intuse)
Scoring$y <- factor(Scoring$y)
str(Scoring)
```


### exploratory data analysis

Analiz. singolarmente le variabili (barplot e istogramma)

```{r}
par(mfrow=c(2,3), pty="s")
par(cex.axis=1.3,cex.lab=1.3,mar=c(3.5,2,1.5,1))
with(Scoring, barplot(table(y),main="y",col="lightblue"))
with(Scoring, barplot(table(account),main="account",col="lightblue"))
with(Scoring, barplot(table(moral),main="moral",col="lightblue"))
with(Scoring, barplot(table(intuse),main="intuse",col="lightblue"))
with(Scoring, hist.scott(duration,main="duration", xlab="", col="lightblue") )
with(Scoring, hist.scott(amount,main="amount", xlab="", col="lightblue") )
par(mfrow=c(1,1))
```


```{r}
layout(matrix(c(1:5,5),byrow=TRUE,nrow=2))
par(cex.axis=1.3,cex.lab=1.3,mar=c(3.5,2,1.5,1))
with(Scoring, boxplot(duration~y, main="duration", col="lightblue"))
with(Scoring, boxplot(amount~y, main="amount", col="lightblue"))


with(Scoring, plot(table(y, account),main=""))
with(Scoring, plot(table(y, moral),main=""))
with(Scoring, plot(table(y, intuse),main=""))
par(mfrow=c(1,1))
```

* nei primi casi uso classico boxplot per categorie
* nel secondo, visualizzo la "ripartizione" delle prob. congiunte

NOTA: come per prima, posso ragionare in ottica inferenziale oppure più in termini di predizione

### multiple logistic regression model

```{r}
mod1 <- glm(y~account+duration+amount+moral+intuse, family=binomial, data=Scoring)

prediction1 <- ifelse(
  # stima della media della risposta (usata come input di ifelse per effettuare previs.)
  predict(mod1, Scoring, type='response') > 0.5, 1, 0)

# prendo anche i val osservati
observed <- Scoring$y
```


### confusion matrix and prediction diagnostic

costruz. della confusion matrix:

```{r}
confusion <- table(prediction1, observed)
colnames(confusion) <- c("creditworthy","not creditworthy")
rownames(confusion) <- c("creditworthy","not creditworthy")
confusion
```

In R con pacchetto crossval posso usare la funz. "confusionMatrix"

(stesso res. ma messo in riga)

```{r}
library(crossval)
cm <- confusionMatrix(Scoring$y, prediction1, negative="0")
cm
```

Con funz. diagnosticErrors invece posso valutare 

```{r}
de <- diagnosticErrors(cm)
de
```
* accurat. globlae
* TPR (sensibilità)
* TNR (specificita)
* valore predittivo positivo
* valore predittivo negativo
* log odds rate


### confusion matrix and prediction diagnostic using cross-validation

Fino adesso abbiamo utilizzato stesso set per training e test. Ora voglio una valutaz. un po' meno ottimista sul modello

Def. la funzione che uso ripetutamente per testare il modello e generare i val. della conf. matrix
```{r}
predfun.glm = function(train.x, train.y, test.x, test.y, negative)
{
  train.data <- data.frame(train.x, train.y)
  glm.fit <- glm(train.y~., binomial, train.data)
  ynew <- predict(glm.fit, newdata=data.frame(test.x, test.y), type="response")
  
  # il valore è magg. di 0.5? --> [1,0]
  ynew <- as.numeric(ynew>0.5)
  out <- confusionMatrix(test.y, ynew, negative=negative)
  return(out)
}
```

```{r}
set.seed(1992)
cv.out <- crossval(predfun.glm, Scoring[,4:8], Scoring$y, 
                   K=10, # ripeti 10 volte 
                   B=1, negative="0", verbose=FALSE)
```


Anche sulla cv posso fare diagnostiche (medie):
```{r}
de.cv<-diagnosticErrors(cv.out$stat)
de.cv
```


```{r}
tabe <- rbind(de[c(1,3,2)],de.cv[c(1,3,2)])
rownames(tabe)<-c("training data","cross-validated")
colnames(tabe)<-c("tot. accuracy","true negative","true positive")
tabe
```


### ROC curve

```{r}
library(verification)
roc.plot(as.numeric(Scoring$y)-1, fitted(mod1), 
    xlab='false positive rate = 1-true negative rate', ylab='true positive rate')

# segmenti  su soglie 0.5
segments(1-de[3],-0.05,1-de[3],de[2],lty=2,col=2,lwd=2)
segments(-0.05,de[2],1-de[3],de[2],lty=2,col=2,lwd=2)
```


### LDA

Classif. con linear discriminant analysis 

```{r}
z <- lda(y ~ amount + duration, Scoring)

prob.lda <- predict(z)$posterior[,2] # [...]
```

Nota: 

* var espl <- devono essere di tipo numerico
* distrib. di prob. di tipo gaussiano

### kNN using the library "RWeka"

Weka <- strumento molto usato, Java

RWeka <- per accederci tramite R

```{r}
library(RWeka)

# res. di kNN (ottenuto tramie Weka)
classifier <- IBk(y ~ moral + intuse + account + amount + duration, data = Scoring, 
                  
                  control = Weka_control(K = 20, # "scegli k ottimale da 1 a 20" 
                                         X= TRUE # (con proc. di cross validation 
                                         )) 


prob.knn <- predict(classifier, 
                    type="probability" # vogliamo prob. esito, non esito
                    )[,2]
```


Confrontiamo LDA, kNN e reg. logistica classica:
```{r}
# verify <- a fronte dei val oss. e previsione, mi fornisce elementi 
# necessari per costr. curva ROC
# (ciò che cambia è il modo in cui ottengo le predizioni)
ver_mod1<-verify(as.numeric(Scoring$y)-1, fitted(mod1), bins = FALSE, show = FALSE)
ver_lda<-verify(as.numeric(Scoring$y)-1, prob.lda, bins = FALSE, show = FALSE)
ver_knn<-verify(as.numeric(Scoring$y)-1, prob.knn, bins = FALSE, show = FALSE)

# ora facciamo il plot dei vari ROC
roc.plot(ver_mod1,xlab='false positive rate', ylab='true positive rate',lwd=2,
         show.thres = FALSE)
lines.roc(ver_lda, col = 2, lwd = 2)  # sovrapponi curva su plot ROC esistente
lines.roc(ver_knn, col = 4, lwd = 2)
```


### Approf. miglior classificatore

kNN <- è risultato il migliore

Verifichiamo i vari tassi

evaluate_Weka_classifier <- funz. che valuta un classificatore.

```{r}

# non voglio CV (di fatto valuto in modo "secco" il set di trainig)
tr.knn <- evaluate_Weka_classifier(classifier, numFolds=0)$confusionMatrix 

# valutaz. meno ottimistica usand cv
cv.knn <- evaluate_Weka_classifier(classifier, numFolds=20, 
              seed=1973)$confusionMatrix 
```


Def. una funzione che calcola i vari param. che  mi interessano data la confusion matrix:
```{r}
ev.acc <- function(CM)
{
  acc <- (CM[1,1] + CM[2,2])/sum(CM) # accuratezza
  tn <-  CM[1,1] / (CM[1,1]+CM[1,2]) # specificità
  tp <-  CM[2,2] / (CM[2,1]+CM[2,2]) # sensibilità
  
  return(c(acc,tn,tp))
}
```


```{r}
tabe1 <- rbind(ev.acc(tr.knn), ev.acc(cv.knn))
rownames(tabe1)<-c("training data","cross-validated")
colnames(tabe1)<-c("tot. accuracy","true negative","true positive")
tabe1
```

Oss: con CV abb

## Example: two-predictors simulated data

Data set "mixture.example" of the package "ElemStatLearn", associated to the book "The Elements of Statistical Learning, Data Mining, Inference, and Prediction" by T. Hastie, J.H. Friedman and R. Tibshirani


```{r}
library(ElemStatLearn)

# estraggo le varie [...]
x <- mixture.example$x
g <- mixture.example$y
xnew <- mixture.example$xnew
px1 <- mixture.example$px1
px2 <- mixture.example$px2

# prob. esito 1, condizionato da px1 e px2
prob.bayes <- matrix(mixture.example$prob, length(px1), length(px2)) 

```

### Bayes decision boundary

Costruiamo la frontiera usando "contour" (disegna una funz. tridimensionale ragionando con le linee di livello):
```{r}
# uso una sola linea di lv (0.5, quella della f. bayesiana)
contour(px1, px2, prob.bayes, levels=0.5, 
        labels="", xlab="x1", axes=FALSE,
        ylab="x2", main="Bayes decision boundary", col="blue")

# sovrapp. i val osservati come punti (distinguendoli per colore)
points(x, col=ifelse(g==1,"red", "black"))

# sovrapp. un box per racchiudere tutto
box()
```

NOTA: anche la front. bayesiana non è perfetta (rimane comunque il classif ottimale)

NOTA: essendo questi dati nati da simulaz. conosco esattamente questi dati (di solito non è così)

### logistic and linear classification boundaries 

Costruiarmo un mod di reg logistica e modello lineare
```{r}
# reg. logistico
mod <- glm(g~x, binomial)
beta <- coef(mod) # coeff. stimati


# creiamo anche mod lineare
mod.lm <- lm(g~x)
beta.lm <- coef(mod.lm) 

```

```{r}

# frontiera bayesiana
contour(px1, px2, prob.bayes, levels=0.5, labels="", xlab="x1", axes=FALSE, 
        ylab="x2", main="Logistic regression boundary", col="blue", lwd=1)

# le oss
points(x, col=ifelse(g==1,"red", "black"))

# Le due frontiere (reg. logistico e lineare)
# (disegno manualmente i punti di inizio e fine)
abline(a=-beta[1]/beta[3], b=-beta[2]/beta[3], lwd=2)
abline(a=-beta.lm[1]/beta.lm[3]+0.5/beta.lm[3], 
       b=-beta.lm[2]/beta.lm[3], col="red", lwd=2)

box()
```


### classification using LDA and QDA (analisi discriminante)

```{r}
x1<-x[,1]
x2<-x[,2]
lda.fit<-lda(g~x1+x2)
lda.fit
```

Oss:
* a priori, le probabilità dei gruppi sono uguali
* medie dei gruppi (condizionate)
* coefficenti lineari da appl. alle var x1 e x2 (nota: segno ci dice come vanno ad incidere su lv della frontiera)


Sull'oggetto lda fit posso fare un plot per vedere come cambia la f. discriminante quando passo da un gruppo all'altro
```{r}
plot(lda.fit)
```


```{r}
lda.pred<-predict(lda.fit,newdata=data.frame(x1=xnew[,1],x2=xnew[,2]))
problda<-matrix(1-lda.pred$posterior,length(px1),length(px2))
# 1-posterior probability, since the class labels are switched
```

Prob. a posteriori 

Perchè 1-...? perchè "posterior" mi da le prob. inverse

(proc analoga per qda)

```{r}
qda.fit<-qda(g~x1+x2)
qda.fit
qda.pred<-predict(qda.fit,newdata=data.frame(x1=xnew[,1],x2=xnew[,2]))
probqda<-matrix(1-qda.pred$posterior,length(px1),length(px2))
# 1-posterior probability, since the class labels are switched
```


```{r}
# decision boundary bayesiano
contour(px1, px2, prob.bayes, levels=0.5, labels="", xlab="x1", axes=FALSE,
        ylab="x2", main="Linear and quadratic discriminant analysis", col="blue",lwd=2)

# 
contour(px1, px2, problda, levels=0.5,labels="", xlab="", axes=FALSE,
        ylab="", main="", co="red", lwd=2, 
        add=T # per sovrascrivere grafico prec. 
        )
contour(px1, px2, probqda, levels=0.5,labels="", xlab="", axes=FALSE,
        ylab="", main="", lwd=2,add=T)


points(x, col=ifelse(g==1,"red", "black"),pch=16)
box()
```



### classification with kNN using the library "class"

Nell'es. prec. usavamo la libreria WikaR

Qui usiamo la lib class e la funz. knn di questa libreria:

#### k=1
```{r}
library(class)

# argomenti:
# - x <- matrice a due col. con val di training (di x1 e x2)
# - g <- val osservati
# - xnew <- var esplicative dei valori che vogli produrre
# - prob=TRUE <- voglio la freq relativa - la prob - associata al valore predetto (non necessariamente sempre di 0 o 1, ma della categoria "predetta")
mod1 <- knn(x, xnew, g, k=1, prob=TRUE)


prob <- attr(mod1, "prob")
prob <- ifelse(mod1=="1", prob, 1-prob) # since KNN produces the probabilities of the predicted class (0 or 1)
prob1 <- matrix(prob, length(px1), length(px2))
```


```{r}
contour(px1, px2, prob1, levels=0.5, labels="", axes=FALSE, 
        main="1-nearest neighbors", xlab="x1", ylab="x2")


contour(px1, px2, prob.bayes, levels=0.5, labels="", xlab="x1",
    ylab="x2", col="blue", lwd=1,add=T)

# coloro i punti in base al val oss.
points(x, col=ifelse(g==1, "red", "black"))

# riempio le soglie colorando i punti di una griglia fine
gd <- expand.grid(x=px1, y=px2)
points(gd, pch=".", cex=1.2, col=ifelse(prob1>0.5, "red", "black"))
box()
```

Nota: oss. probl. di overfitting

#### k=10

```{r}
mod10 <- knn(x, xnew, g, k=10, prob=TRUE)
prob <- attr(mod10, "prob")
prob <- ifelse(mod10=="1", prob, 1-prob)
prob1 <- matrix(prob, length(px1), length(px2))
contour(px1, px2, prob1, levels=0.5, labels="", axes=FALSE, 
        main="10-nearest neighbors", xlab="x1", ylab="x2")
points(x, col=ifelse(g==1, "red", "black"))
gd <- expand.grid(x=px1, y=px2)
points(gd, pch=".", cex=1.2, col=ifelse(prob1>0.5, "red", "black"))
contour(px1, px2, prob.bayes, levels=0.5, labels="", xlab="x1", ylab="x2", 
        col="blue", lwd=1,add=T)
box()
```


#### k=50

```{r}
mod50 <- knn(x, xnew, g, k=50, prob=TRUE)
prob <- attr(mod50, "prob")
prob <- ifelse(mod50=="1", prob, 1-prob)
prob1 <- matrix(prob, length(px1), length(px2))
contour(px1, px2, prob1, levels=0.5, labels="", axes=FALSE, 
        main="50-nearest neighbors",xlab="x1", ylab="x2")
points(x, col=ifelse(g==1, "red", "black"))
gd <- expand.grid(x=px1, y=px2)
points(gd, pch=".", cex=1.2, col=ifelse(prob1>0.5, "red", "black"))
contour(px1, px2, prob.bayes, levels=0.5, labels="", xlab="x1", ylab="x2", 
        col="blue", lwd=1,add=T)
abline(a=-beta[1]/beta[3], b=-beta[2]/beta[3], col=2, lwd=2)
box()
```

Commento: 

knn <- metodo molto generale, si può applicare con regressori di vario tipo (e più categorie)

problema <- dare una nozione di distanza efficade 

* v. numeriche <- dist euclidea

 
















