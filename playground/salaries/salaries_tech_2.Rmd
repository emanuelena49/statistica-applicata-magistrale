---
title: "Salaries in tech industry"
author: "Emanuele Lena - 142411@uniud"
date: "30/11/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(lubridate)
library(dplyr)
```


## Load dataset + overview

Load dataset:
```{r}
salaries <- read.csv("Levels_Fyi_Salary_Data.csv")
head(salaries, 50) 

```

### Number of observations

Number of observations (salaries of single employee):
```{r}
nrow(salaries)
```

### Overview on variables

```{r}
summary(salaries)
```

#### Results (variable meaning)
* I notice two type of variables: the ones related to "what the employee gets" and all others;
* variables related to what employer gets are: 

"basesalary", "totalyearlycompensation", "bonus", "stockgrantvalue";

* the other variables include informations like:

** company
** job position
** physical location
** employee experience (in company and globally)
** education
** gender
** race

* each row is also associated to a timestamp (the source date of the data?).

#### Results (variable types and need for tidy)

From summary call, I notice some variables seems "well typed", while others seems just strange and redundant. In detail:

* date is formatted in an unusual format, difficult to handle for ordering 
  -> I will need to to put it in the usual format "yyyy/mm/dd hh:mm:ss"
  
* there are a lot of redundant variables (int to interpretate as booleans?) for race and education
  -> I remove all them and keep only "Race" and "Education"
  
* It's not so much clear the meaning of "otherdetails", let's endept it:

```{r}
(salaries[!is.na(salaries$otherdetails), "otherdetails"])[1:5]
```

  -> From what I see, it seems just an optional free text. Let's keep it (but ignore at first).
  
* there are some variabiles which rappresent ids (naturally "categorial") but are seen as numerical ("cityid", "dmaid")
  -> I force them factors to become factors
  
* for some numerical variables, it looks I have a lot of zeros that may be used as "NA" (fairly plausible because of the origin of dataset)
  -> I cannot know, so for now I ignore it
 
 
## Data cleaning 

Create a copy of data:
```{r}
salaries2 <- salaries

```

### Datet cleaning

Do the datetimes cleaning (using lubridate) + add pure dates and years

```{r}
# library(lubridate)

# override timestamp using my new format
salaries2$timestamp <- mdy_hms(salaries$timestamp)

# I suppose I will not need the full datetime, so I parse just the date (and add as new col)
salaries2$date <- as.Date(substr(salaries2$timestamp, 0, 10))

salaries2$year <- year(salaries2$timestamp)

```

### Remove unused variables

```{r}
salaries2 <- salaries2 %>% 
  select(-Race_Asian, -Race_White, -Race_Two_Or_More, -Race_Black, -Race_Hispanic, 
         -Masters_Degree, -Some_College, -Doctorate_Degree, -Highschool, -Bachelors_Degree)
```

### Numerical -> factors

```{r}
salaries2$cityid <- factor(salaries2$cityid)
salaries2$dmaid <- factor(salaries2$dmaid)
```

From what I seen during overview, I noticed those factors has no NA value, but they have zeros. Let's endeep that:

```{r}
length(salaries2[salaries2$cityid==0, "cityid"]) 
length(salaries2[salaries2$dmaid==0, "dmaid"]) 

```

In "cityid" I have only two zeros, so let's assume they are valid ids.

In "dmaid" I have lots of them. For now let's keep them as valid values, but remember it for future.

### Clean dataset structure

```{r}
summary(salaries2)
```

(Evidently, someone in Big Tech companies identifies sexually as a senior software engineer)

```{r}
table(salaries2$gender) %>% as.data.frame()
```

## An overview on total year compensation

Total year compensation rappresent the total pay an employee recevies in a year. It will be mine main focus for the next analysis. The total compensation is the sum of all forms of payments, like:
* the regular salary
* bonuses
* stock options
* etc.

This variable will be the main response variable I wanna study and predict.

### Checksum propery 

According to the meaning of TYC, I expect the sum of the three variables "basesalary", "bonus", "stockgrantvalue" equals to "totalyearlycompensation". 

On the other side, as seen in the initial quick table print, I have a lot of "zeros" in the three hypotetic variables (may they be NA?). 

Let's do a quick check calculating a checksum on each rows (calculate difference tyc - this and check if it is != 0)

```{r}
diffs <- with(salaries2, totalyearlycompensation - basesalary - bonus - stockgrantvalue)

diffs[1:44]

table(diffs == 0)

```

Let's focus on rows where all secondary values are != 0:

```{r}
salaries_3v_all_not_0 <- salaries2 %>% filter(basesalary!=0 & bonus!=0 & stockgrantvalue!=0)

diffs2 <- with(salaries_3v_all_not_0, totalyearlycompensation - basesalary - bonus - stockgrantvalue)

diffs2[1:44]

table(diffs2 == 0)

```


Remember nrows:

```{r}
nrow(salaries2)
```

#### Results

In a decent amount of rows (50198/62642), the checksum confirms the theory but not all data are uniform.

### Total year compensation statistics

Let's get a quick view on t.y.c.

```{r}
summary(salaries2$totalyearlycompensation)
```


### Total year compensation density and Gauss model hypotesis

Let's endept t.y.c. density and make some hypotesis on an eventual appropriate model.

```{r}
# rename "totalyearlycompensation" as "tyc" as shortcut
salaries2$tyc <- salaries2$totalyearlycompensation
```


```{r}
hist(salaries2$tyc, breaks=seq(0, 5000000, 50000))
rug(salaries2$tyc, col="lightblue")
```

I notice t.y.c. is clerly not a Gauss distribution, because:

* there is a clear form of assimmetry (as already seen from statistics)
* there are some really extreme values on right side
* most of the data seems concentrated on the left side of curve, but right tail is still somewhat heavy

Let's try two different approaches to endept the phenom: 

1) cut out all values over a certain value (let's take the 0.95 quantile)

```{r}
quantile(salaries2$tyc, 0.95)
```

2) keep all values but analyze log(tyc) instead of tyc.

In both cases I plot a dnorm curve and an adaptive curve over the histogram to look for normal distribution hints.

```{r}

gaussOverData <- function(data, xDensity=5000, color="red") {
  
  x <- seq(min(data), max(data), by=(max(data)-min(data))/xDensity)
  y <- dnorm(x, mean(data), sd(data))
  
  return(lines(x, y, lwd=2, col=color))
}


```


```{r}
par(mfrow=c(1,2))

# -------------------------
# Hist filtered on obs. <= 0.95 quantile

salaries_under_500k <- salaries2 %>% filter(tyc <= 500000)

hist(salaries_under_500k$tyc, 
     breaks=seq(0, 500000, 50000), 
     freq = F, col="lightgray",  
     main="Histogram under 0.95 quantile", xlab="tyc")
gaussOverData(salaries_under_500k$tyc)
lines(density(salaries_under_500k$tyc))

# -------------------------
# Log scale hist

salaries2$logtyc <- log(salaries2$tyc)

hist(salaries2$logtyc, freq = F, col="lightgray", 
     main="Histogram of log(tyc)", xlab="log(tyc)")
rug(salaries2$logtyc, col="lightblue")
gaussOverData(salaries2$logtyc)
lines(density(salaries2$logtyc))

par(mfrow=c(1,1))
```


#### Results

From what I see, the whole data density is fairly different from what I expect from a Gauss model. Removing the largest 5% of data, things seems better, but:

* i still observe an apparent left assymmetry;
* i still have a too much heavy right tail. 

Trasforming the whole data in a log scale, makes thing looks slighly better, even if not either this looks like a normal distribution.

Let's validate those considerations first with a quantile analysis, then using some proper mathematical test.

### Quantiles & outlyers

Let's start with a quick overview of quartiles and outlyers using boxplots:

```{r}
par(mfrow=c(1,2))

boxplot(salaries2$tyc, ylab="tyc")

boxplot(salaries2$logtyc, ylab="log(tyc)")

par(mfrow=c(1,1))

```


#### Results

In both boxplots, I notice a huge amount of outlyers, but while in the first one they are onesided, in the second one this seems quite symmetric.

Let's endept with a view of quantiles (compared with the one of a normal distribution).

```{r}
par(mfrow=c(1,3))

qqnorm(salaries2$tyc, main = "Normal Q-Q Plot (tyc)")
qqline(salaries2$tyc,col='red',lwd=2)

qqnorm(salaries_under_500k$tyc, main = "Normal Q-Q Plot (tyc<=500k)")
qqline(salaries_under_500k$tyc,col='red',lwd=2)

qqnorm(salaries2$logtyc, main = "Normal Q-Q Plot (log tyc)")
qqline(salaries2$logtyc,col='red',lwd=2)


par(mfrow=c(1,1))

```

### Normality tests

[...]


## Overview on explicative variables

### Titles and levels

```{r}
summary(salaries2$title)
```

```{r}
summary(salaries2$level) %>% as.data.frame()
```

```{r}

ggplot(table(salaries2$title) %>% as.data.frame(col.names=c("title", "f")), 
                aes(x=reorder(title, -f), f)) + geom_bar(stat ="identity")

```

